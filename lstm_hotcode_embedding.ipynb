{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99685\n",
      "nn prp i vbd to dt an A nn nn\n",
      "jj nn innn on prpd your jj nn\n"
     ]
    }
   ],
   "source": [
    "#correctSentFile = \"C:\\\\Python364\\\\Scripts\\\\Grammar Corrector\\\\5-combined-tagged-words-cor-sent1.csv\"\n",
    "#correctSentFile = \"C:\\\\Python364\\\\Scripts\\\\Grammar Corrector\\\\combined-tagged-correct-sent3.csv\"\n",
    "correctSentFile = \"combined_tagged_lang_selected_entries.csv\"\n",
    "corrSent = pd.read_csv(correctSentFile,header=None,sep=\"\\t\")\n",
    "\n",
    "#corrSent = corrSent[4]\n",
    "corrSent = corrSent[2]\n",
    "tempSent = []\n",
    "print(len(corrSent))\n",
    "for sent in corrSent:\n",
    "    sent = sent.strip()\n",
    "    sent = re.sub(r'\\.{1,}',r'',sent)\n",
    "    tempSent.append(sent)\n",
    "corrSent = tempSent\n",
    "  \n",
    "corrSent = []\n",
    "\n",
    "bl = list(range(len(tempSent)))\n",
    "random.shuffle(bl)\n",
    "for j in bl:\n",
    "    corrSent.append(tempSent[j])\n",
    "    \n",
    "print(corrSent[0])\n",
    "print(tempSent[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt a A dt a L jj nn\n",
      "jj nn innn at prpd your jj nn\n",
      "92082\n"
     ]
    }
   ],
   "source": [
    "inCorrectSentFile = \"combined_tagged_specific_error_lang_selected_entries.csv\"\n",
    "IncorrSent = pd.read_csv(inCorrectSentFile,header=None,sep=\"\\t\")\n",
    "IncorrSent = IncorrSent[2]\n",
    "tempSent = []\n",
    "#print(len(IncorrSent))\n",
    "for sent in IncorrSent:\n",
    "    sent = sent.strip()\n",
    "    sent = re.sub(r'\\.{1,}',r'',sent)\n",
    "    tempSent.append(sent)\n",
    "    \n",
    "inCorrectSentFile = \"LINK removed for Security reasons\"\n",
    "\n",
    "IncorrSent = pd.read_csv(inCorrectSentFile,header=None,sep=\"\\t\")\n",
    "#IncorrSent = IncorrSent[0]\n",
    "IncorrSent = IncorrSent[2]\n",
    "\n",
    "#print(len(IncorrSent))\n",
    "for sent in IncorrSent:\n",
    "    sent = sent.strip()\n",
    "    sent = re.sub(r'\\.{1,}',r'',sent)\n",
    "    tempSent.append(sent)\n",
    "#IncorrSent = tempSent\n",
    "\n",
    "IncorrSent = []\n",
    "bl = list(range(len(tempSent)))\n",
    "random.shuffle(bl)\n",
    "for j in bl:\n",
    "    IncorrSent.append(tempSent[j])\n",
    "    #shuffledIndexFile.write(str(j)+\"\\n\")\n",
    "print(IncorrSent[0])\n",
    "print(tempSent[0])\n",
    "print(len(tempSent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prp it vbd rb jj jj' '0']\n"
     ]
    }
   ],
   "source": [
    "corrSent = np.array(corrSent)\n",
    "zeros = np.zeros((len(corrSent),1),dtype=int) #creating labels of correct sentences\n",
    "corrSent = np.reshape(corrSent,(len(corrSent),1))\n",
    "corrSent = np.append(corrSent,zeros,axis=1) #\n",
    "print(corrSent[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dt a A dt a L jj nn' '1']\n"
     ]
    }
   ],
   "source": [
    "IncorrSent = np.array(IncorrSent)\n",
    "ones = np.ones((len(IncorrSent),1),dtype=int) #creating labels of incorrect sentences\n",
    "IncorrSent = np.reshape(IncorrSent,(len(IncorrSent),1))\n",
    "IncorrSent = np.append(IncorrSent,ones,axis=1)\n",
    "print(IncorrSent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000\n",
      "96000\n"
     ]
    }
   ],
   "source": [
    "#partition for specific errors set\n",
    "'''\n",
    "trainSet = np.append(corrSent[0:40000,],IncorrSent[0:25000,],axis=0)\n",
    "trainSet = np.append(trainSet[0:65000,],IncorrSent[10000:25000,],axis=0)\n",
    "testSet = np.append(corrSent[40000:50000,],IncorrSent[25000:31623,],axis=0)\n",
    "testSet = np.append(testSet[0:16623,],IncorrSent[11000:14377,],axis=0)\n",
    "'''\n",
    "#partition for combined errors set\n",
    "trainSet = np.append(corrSent[0:48000,],IncorrSent[0:24000,],axis=0)\n",
    "trainSet = np.append(trainSet[0:72000,],IncorrSent[40000:64000,],axis=0)\n",
    "testSet = np.append(corrSent[48000:60000,],IncorrSent[24000:30000,],axis=0)\n",
    "testSet = np.append(testSet[0:18000,],IncorrSent[64000:70000,],axis=0)\n",
    "\n",
    "'''\n",
    "#partition for general errors set\n",
    "trainSet = np.append(corrSent[0:48000,],IncorrSent[0:48000,],axis=0)\n",
    "testSet = np.append(corrSent[48000:60000,],IncorrSent[48000:60000,],axis=0)\n",
    "'''\n",
    "print(len(testSet))\n",
    "print(len(trainSet))\n",
    "print(len(trainSet))\n",
    "print(len(testSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prp he vbd dt a K nn innn on dt the nn' '0']\n"
     ]
    }
   ],
   "source": [
    "print(testSet[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "def makeSentEqLen(dataset,length=None):\n",
    "    tempSet=[]\n",
    "    labels = []\n",
    "    if length is None:\n",
    "        \n",
    "        length = max([len(sentence.split()) for sentence in dataset])\n",
    "        print(length)\n",
    "    for i in range(len(dataset)):\n",
    "        tempSent = dataset[i,0]\n",
    "        totalWords = len(tempSent.split())\n",
    "        if(totalWords>length):continue\n",
    "        for j in range((length - totalWords)):\n",
    "            #print(tempSent)\n",
    "            tempSent = tempSent + \" end\"\n",
    "        tempSet.append(tempSent)\n",
    "        labels.append(dataset[i,1])\n",
    "        #print(dataset[i,1])\n",
    "        \n",
    "      \n",
    "    return tempSet,labels\n",
    "seqLen = 24 #general errors\n",
    "#seqLen = 20 #specific errors\n",
    "equalLenTrainSet,trainLabels = makeSentEqLen(trainSet,seqLen)\n",
    "print(max([len(sentence.split()) for sentence in trainSet[:,0]]))\n",
    "#print(trainSet[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96000\n",
      "96000\n",
      "['nn prp i vbd to dt an A nn nn end end end end end end end end end end end end end end', 'prp i rb vbd prp it end end end end end end end end end end end end end end end end end end', 'prp it vbd rb jj jj end end end end end end end end end end end end end end end end end end', 'vb prp you vbg  end end end end end end end end end end end end end end end end end end end end', 'nnp , nns end end end end end end end end end end end end end end end end end end end end end']\n"
     ]
    }
   ],
   "source": [
    "print(len(equalLenTrainSet))\n",
    "print(len(trainLabels))\n",
    "print(equalLenTrainSet[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('C:\\Python364\\Scripts\\Grammar Corrector\\datasets\\GoogleNews-vectors\\GoogleNews-vectors-negative300.bin', binary=True,limit=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00524902 -0.14355469 -0.06933594  0.12353516  0.13183594 -0.08886719\n",
      " -0.07128906 -0.21679688 -0.19726562  0.05566406]\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_model.word_vec(\"queen\")[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 2, 'B': 3, 'C': 4, 'D': 5, 'E': 6, 'F': 7, 'G': 8, 'H': 9, 'I': 10, 'J': 11, 'K': 12, 'L': 13, 'M': 14, 'N': 15, 'O': 16, 'P': 17, 'Q': 18, 'R': 19, 'S': 20, 'T': 21, 'U': 22, 'V': 23, 'W': 24, 'X': 25, 'Y': 26, 'Z': 27, 'prp': 28, 'nn': 29, 'innn': 30, 'dt': 31, 'jj': 32, 'vbz': 33, 'vbd': 34, 'rb': 35, 'vbp': 36, 'vb': 37, 'nnp': 38, 'nns': 39, 'vbg': 40, 'prpd': 41, 'md': 42, 'vbn': 43, 'wp': 44, 'cd': 45, 'rp': 46, 'wrb': 47, 'pos': 48, 'cc': 49, 'jjr': 50, 'jjs': 51, 'ex': 52, 'to': 53, 'rbr': 54, ',': 55, \"'\": 56, 'a': 57, 'an': 58, 'the': 59}\n"
     ]
    }
   ],
   "source": [
    "posTagListFile = \"pos-tag.csv\"\n",
    "posTagList = pd.read_csv(posTagListFile,header=None,sep=\"\\t\")\n",
    "posTag = posTagList[0]\n",
    "posTag_int = {word: ii for ii, word in enumerate(posTag, 2)}\n",
    "print(posTag_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hotEmbedFeatures = []\n",
    "*********************************
    
    
    
    Removed for security reasons, this detialed code will be provided upon request !
    
    
    
    *****************************************--
    "\n",
    "hotEmbedFeatures = np.reshape(hotEmbedFeatures,(len(hotEmbedFeatures),seqLen,embedLength))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96000, 24, 75)\n",
      "(96000,)\n",
      "(96000, 24, 75)\n",
      "(96000,)\n"
     ]
    }
   ],
   "source": [
    "print(hotEmbedFeatures.shape)\n",
    "trainLabels = np.array(trainLabels)\n",
    "print(trainLabels.shape)\n",
    "#hotEmbedFeatures = hotEmbedFeatures[0:1190,]\n",
    "#trainLabels = trainLabels[0:1190,]\n",
    "print(hotEmbedFeatures.shape)\n",
    "print(trainLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0120 03:13:01.141938  2752 deprecation_wrapper.py:119] From C:\\Python364\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0120 03:13:01.651139  2752 deprecation_wrapper.py:119] From C:\\Python364\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0120 03:13:01.746588  2752 deprecation_wrapper.py:119] From C:\\Python364\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0120 03:13:02.014865  2752 deprecation_wrapper.py:119] From C:\\Python364\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0120 03:13:02.021849  2752 deprecation.py:506] From C:\\Python364\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0120 03:13:02.483342  2752 deprecation_wrapper.py:119] From C:\\Python364\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0120 03:13:02.507463  2752 deprecation_wrapper.py:119] From C:\\Python364\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (4000, 24, 50)            25200     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (4000, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (4000, 64)                3264      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (4000, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (4000, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (4000, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 51,026\n",
      "Trainable params: 51,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense,Dropout,InputLayer,Embedding\n",
    "from keras.layers.core import Flatten\n",
    "def lstmModel(batchSize):\n",
    "    \n",
    "    model = Sequential()\n",
    "    batch_size = batchSize\n",
    "    #model.add(Embedding(80, 10, input_length=seqLen))\n",
    "    #model.add(LSTM(20,return_sequences=True,dropout=0.1,recurrent_dropout=0.1))#output dimensions batch_size,timesteps,20\n",
    "    model.add(LSTM(50,batch_input_shape=(batch_size,seqLen,embedLength),return_sequences=True,dropout=0.1,recurrent_dropout=0.1))#output dimensions batch_size,timesteps,20\n",
    "    model.add(LSTM(50))\n",
    "    #model.add(Flatten())\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(32,activation='relu'))\n",
    "    model.add(Dense(8,activation='relu'))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "    return model\n",
    "model = lstmModel(4000)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0120 03:13:02.790677  2752 deprecation.py:323] From C:\\Python364\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "96000/96000 [==============================] - 26s 275us/step - loss: 0.6855 - acc: 0.5520\n",
      "Epoch 2/500\n",
      "96000/96000 [==============================] - 23s 238us/step - loss: 0.6656 - acc: 0.5912\n",
      "Epoch 3/500\n",
      "96000/96000 [==============================] - 23s 237us/step - loss: 0.6507 - acc: 0.6127\n",
      "Epoch 4/500\n",
      "96000/96000 [==============================] - 23s 241us/step - loss: 0.6393 - acc: 0.6237\n",
      "Epoch 5/500\n",
      "96000/96000 [==============================] - 24s 245us/step - loss: 0.6297 - acc: 0.6296\n",
      "Epoch 6/500\n",
      "96000/96000 [==============================] - 23s 237us/step - loss: 0.6239 - acc: 0.6336\n",
      "Epoch 7/500\n",
      "96000/96000 [==============================] - 23s 236us/step - loss: 0.6196 - acc: 0.6354\n",
      "Epoch 8/500\n",
      "96000/96000 [==============================] - 23s 236us/step - loss: 0.6148 - acc: 0.6402\n",
      "Epoch 9/500\n",
      "96000/96000 [==============================] - 23s 237us/step - loss: 0.6109 - acc: 0.6427\n",
      "Epoch 10/500\n",
      "96000/96000 [==============================] - 23s 243us/step - loss: 0.6050 - acc: 0.6467\n",
      "Epoch 11/500\n",
      "96000/96000 [==============================] - 25s 260us/step - loss: 0.6010 - acc: 0.6497\n",
      "Epoch 12/500\n",
      "96000/96000 [==============================] - 24s 246us/step - loss: 0.5970 - acc: 0.6548\n",
      "Epoch 13/500\n",
      "96000/96000 [==============================] - 22s 231us/step - loss: 0.5921 - acc: 0.6562\n",
      "Epoch 14/500\n",
      "96000/96000 [==============================] - 22s 231us/step - loss: 0.5862 - acc: 0.6619\n",
      "Epoch 15/500\n",
      "96000/96000 [==============================] - 23s 236us/step - loss: 0.5801 - acc: 0.6667\n",
      "Epoch 16/500\n",
      "96000/96000 [==============================] - 22s 226us/step - loss: 0.5756 - acc: 0.6727\n",
      "Epoch 17/500\n",
      "96000/96000 [==============================] - 22s 227us/step - loss: 0.5707 - acc: 0.6764\n",
      "Epoch 18/500\n",
      "96000/96000 [==============================] - 23s 238us/step - loss: 0.5658 - acc: 0.6808\n",
      "Epoch 19/500\n",
      "96000/96000 [==============================] - 22s 231us/step - loss: 0.5624 - acc: 0.6829\n",
      "Epoch 20/500\n",
      "96000/96000 [==============================] - 22s 232us/step - loss: 0.5569 - acc: 0.6886\n",
      "Epoch 21/500\n",
      "96000/96000 [==============================] - 22s 234us/step - loss: 0.5528 - acc: 0.6910\n",
      "Epoch 22/500\n",
      "96000/96000 [==============================] - 22s 233us/step - loss: 0.5512 - acc: 0.6938\n",
      "Epoch 23/500\n",
      "96000/96000 [==============================] - 24s 245us/step - loss: 0.5478 - acc: 0.6961\n",
      "Epoch 24/500\n",
      "96000/96000 [==============================] - 23s 240us/step - loss: 0.5466 - acc: 0.6991\n",
      "Epoch 25/500\n",
      "96000/96000 [==============================] - 23s 236us/step - loss: 0.5447 - acc: 0.7012\n",
      "Epoch 26/500\n",
      "96000/96000 [==============================] - 22s 229us/step - loss: 0.5405 - acc: 0.7039\n",
      "Epoch 27/500\n",
      "96000/96000 [==============================] - 22s 229us/step - loss: 0.5384 - acc: 0.7059\n",
      "Epoch 28/500\n",
      "96000/96000 [==============================] - 22s 232us/step - loss: 0.5370 - acc: 0.7079\n",
      "Epoch 29/500\n",
      "96000/96000 [==============================] - 23s 236us/step - loss: 0.5339 - acc: 0.7108\n",
      "Epoch 30/500\n",
      "96000/96000 [==============================] - 22s 229us/step - loss: 0.5312 - acc: 0.7116\n",
      "Epoch 31/500\n",
      "96000/96000 [==============================] - 22s 230us/step - loss: 0.5291 - acc: 0.7140\n",
      "Epoch 32/500\n",
      "96000/96000 [==============================] - 22s 232us/step - loss: 0.5275 - acc: 0.7172\n",
      "Epoch 33/500\n",
      "96000/96000 [==============================] - 22s 229us/step - loss: 0.5262 - acc: 0.7176\n",
      "Epoch 34/500\n",
      "96000/96000 [==============================] - 22s 230us/step - loss: 0.5216 - acc: 0.7225\n",
      "Epoch 35/500\n",
      "96000/96000 [==============================] - 24s 247us/step - loss: 0.5216 - acc: 0.7232\n",
      "Epoch 36/500\n",
      "96000/96000 [==============================] - 22s 234us/step - loss: 0.5176 - acc: 0.7271\n",
      "Epoch 37/500\n",
      "96000/96000 [==============================] - 23s 242us/step - loss: 0.5158 - acc: 0.7267\n",
      "Epoch 38/500\n",
      "96000/96000 [==============================] - 23s 234us/step - loss: 0.5112 - acc: 0.7318\n",
      "Epoch 39/500\n",
      "96000/96000 [==============================] - 22s 233us/step - loss: 0.5096 - acc: 0.7345\n",
      "Epoch 40/500\n",
      "96000/96000 [==============================] - 24s 248us/step - loss: 0.5067 - acc: 0.7380\n",
      "Epoch 41/500\n",
      "96000/96000 [==============================] - 23s 237us/step - loss: 0.5047 - acc: 0.7390\n",
      "Epoch 42/500\n",
      "96000/96000 [==============================] - 24s 247us/step - loss: 0.5004 - acc: 0.7440\n",
      "Epoch 43/500\n",
      "96000/96000 [==============================] - 26s 273us/step - loss: 0.4998 - acc: 0.7456\n",
      "Epoch 44/500\n",
      "96000/96000 [==============================] - 23s 242us/step - loss: 0.4957 - acc: 0.7471\n",
      "Epoch 45/500\n",
      "96000/96000 [==============================] - 23s 238us/step - loss: 0.4949 - acc: 0.7488\n",
      "Epoch 46/500\n",
      "96000/96000 [==============================] - 22s 230us/step - loss: 0.4901 - acc: 0.7527\n",
      "Epoch 47/500\n",
      "96000/96000 [==============================] - 23s 236us/step - loss: 0.4912 - acc: 0.7516\n",
      "Epoch 48/500\n",
      "96000/96000 [==============================] - 25s 258us/step - loss: 0.4888 - acc: 0.7549\n",
      "Epoch 49/500\n",
      "96000/96000 [==============================] - 25s 257us/step - loss: 0.4845 - acc: 0.7562\n",
      "Epoch 50/500\n",
      "96000/96000 [==============================] - 24s 251us/step - loss: 0.4825 - acc: 0.7588\n",
      "Epoch 51/500\n",
      "96000/96000 [==============================] - 23s 240us/step - loss: 0.4791 - acc: 0.7607\n",
      "Epoch 52/500\n",
      "96000/96000 [==============================] - 22s 233us/step - loss: 0.4781 - acc: 0.7607\n",
      "Epoch 53/500\n",
      "96000/96000 [==============================] - 23s 238us/step - loss: 0.4752 - acc: 0.7630\n",
      "Epoch 54/500\n",
      "96000/96000 [==============================] - 24s 245us/step - loss: 0.4762 - acc: 0.7620\n",
      "Epoch 55/500\n",
      "96000/96000 [==============================] - 22s 230us/step - loss: 0.4721 - acc: 0.7660\n",
      "Epoch 56/500\n",
      "96000/96000 [==============================] - 22s 228us/step - loss: 0.4692 - acc: 0.7680\n",
      "Epoch 57/500\n",
      "96000/96000 [==============================] - 22s 227us/step - loss: 0.4709 - acc: 0.7670\n",
      "Epoch 58/500\n",
      "96000/96000 [==============================] - 22s 234us/step - loss: 0.4658 - acc: 0.7700\n",
      "Epoch 59/500\n",
      "96000/96000 [==============================] - 22s 233us/step - loss: 0.4648 - acc: 0.7702\n",
      "Epoch 60/500\n",
      "96000/96000 [==============================] - 22s 228us/step - loss: 0.4637 - acc: 0.7711\n",
      "Epoch 61/500\n",
      "96000/96000 [==============================] - 23s 235us/step - loss: 0.4586 - acc: 0.7746\n",
      "Epoch 62/500\n",
      "96000/96000 [==============================] - 23s 236us/step - loss: 0.4590 - acc: 0.7738\n",
      "Epoch 63/500\n",
      "96000/96000 [==============================] - 22s 229us/step - loss: 0.4581 - acc: 0.7749\n",
      "Epoch 64/500\n",
      "96000/96000 [==============================] - 22s 230us/step - loss: 0.4575 - acc: 0.7758\n",
      "Epoch 65/500\n",
      "96000/96000 [==============================] - 22s 232us/step - loss: 0.4560 - acc: 0.7765\n",
      "Epoch 66/500\n",
      "96000/96000 [==============================] - 22s 229us/step - loss: 0.4546 - acc: 0.7776\n",
      "Epoch 67/500\n",
      "96000/96000 [==============================] - 22s 229us/step - loss: 0.4507 - acc: 0.7798\n",
      "Epoch 68/500\n",
      "96000/96000 [==============================] - 22s 233us/step - loss: 0.4512 - acc: 0.7795\n",
      "Epoch 69/500\n",
      "96000/96000 [==============================] - 22s 230us/step - loss: 0.4467 - acc: 0.7827\n",
      "Epoch 70/500\n",
      "96000/96000 [==============================] - 22s 233us/step - loss: 0.4488 - acc: 0.7812\n",
      "Epoch 71/500\n",
      "96000/96000 [==============================] - 19s 196us/step - loss: 0.4449 - acc: 0.7840\n",
      "Epoch 72/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.4437 - acc: 0.7858\n",
      "Epoch 73/500\n",
      "96000/96000 [==============================] - 18s 190us/step - loss: 0.4430 - acc: 0.7851\n",
      "Epoch 74/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.4411 - acc: 0.7860\n",
      "Epoch 75/500\n",
      "96000/96000 [==============================] - 18s 190us/step - loss: 0.4391 - acc: 0.7865\n",
      "Epoch 76/500\n",
      "96000/96000 [==============================] - 18s 190us/step - loss: 0.4396 - acc: 0.7869\n",
      "Epoch 77/500\n",
      "96000/96000 [==============================] - 18s 190us/step - loss: 0.4378 - acc: 0.7886\n",
      "Epoch 78/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.4333 - acc: 0.7907\n",
      "Epoch 79/500\n",
      "96000/96000 [==============================] - 18s 190us/step - loss: 0.4363 - acc: 0.7887\n",
      "Epoch 80/500\n",
      "96000/96000 [==============================] - 18s 190us/step - loss: 0.4352 - acc: 0.7901\n",
      "Epoch 81/500\n",
      "96000/96000 [==============================] - 18s 190us/step - loss: 0.4322 - acc: 0.7921\n",
      "Epoch 82/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.4302 - acc: 0.7927\n",
      "Epoch 83/500\n",
      "96000/96000 [==============================] - 18s 190us/step - loss: 0.4305 - acc: 0.7927\n",
      "Epoch 84/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.4288 - acc: 0.7925\n",
      "Epoch 85/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.4279 - acc: 0.7930\n",
      "Epoch 86/500\n",
      "96000/96000 [==============================] - 18s 190us/step - loss: 0.4240 - acc: 0.7960\n",
      "Epoch 87/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.4265 - acc: 0.7952\n",
      "Epoch 88/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.4231 - acc: 0.7962\n",
      "Epoch 89/500\n",
      "96000/96000 [==============================] - 19s 194us/step - loss: 0.4243 - acc: 0.7963\n",
      "Epoch 90/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.4213 - acc: 0.7987\n",
      "Epoch 91/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.4221 - acc: 0.7979\n",
      "Epoch 92/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.4199 - acc: 0.7985\n",
      "Epoch 93/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.4191 - acc: 0.7994\n",
      "Epoch 94/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.4195 - acc: 0.7986\n",
      "Epoch 95/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.4168 - acc: 0.8003\n",
      "Epoch 96/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.4156 - acc: 0.8021\n",
      "Epoch 97/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.4141 - acc: 0.8025\n",
      "Epoch 98/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.4139 - acc: 0.8034\n",
      "Epoch 99/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.4136 - acc: 0.8034\n",
      "Epoch 100/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.4097 - acc: 0.8030\n",
      "Epoch 101/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.4114 - acc: 0.8045\n",
      "Epoch 102/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.4103 - acc: 0.8043\n",
      "Epoch 103/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.4088 - acc: 0.8061\n",
      "Epoch 104/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.4084 - acc: 0.8054\n",
      "Epoch 105/500\n",
      "96000/96000 [==============================] - 19s 194us/step - loss: 0.4072 - acc: 0.8066\n",
      "Epoch 106/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.4074 - acc: 0.8065\n",
      "Epoch 107/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.4075 - acc: 0.8080\n",
      "Epoch 108/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.4027 - acc: 0.8102\n",
      "Epoch 109/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.4053 - acc: 0.8072\n",
      "Epoch 110/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.4041 - acc: 0.8067\n",
      "Epoch 111/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.4027 - acc: 0.8105\n",
      "Epoch 112/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.4011 - acc: 0.8114\n",
      "Epoch 113/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.4002 - acc: 0.8120\n",
      "Epoch 114/500\n",
      "96000/96000 [==============================] - 19s 197us/step - loss: 0.4001 - acc: 0.8102\n",
      "Epoch 115/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3988 - acc: 0.8114\n",
      "Epoch 116/500\n",
      "96000/96000 [==============================] - 19s 195us/step - loss: 0.4003 - acc: 0.8097\n",
      "Epoch 117/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3971 - acc: 0.8123\n",
      "Epoch 118/500\n",
      "96000/96000 [==============================] - 18s 193us/step - loss: 0.3947 - acc: 0.8137\n",
      "Epoch 119/500\n",
      "96000/96000 [==============================] - 19s 197us/step - loss: 0.3977 - acc: 0.8132\n",
      "Epoch 120/500\n",
      "96000/96000 [==============================] - 19s 197us/step - loss: 0.3965 - acc: 0.8135\n",
      "Epoch 121/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3959 - acc: 0.8131\n",
      "Epoch 122/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3945 - acc: 0.8146\n",
      "Epoch 123/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3949 - acc: 0.8138\n",
      "Epoch 124/500\n",
      "96000/96000 [==============================] - 19s 195us/step - loss: 0.3928 - acc: 0.8150\n",
      "Epoch 125/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3928 - acc: 0.8162\n",
      "Epoch 126/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3923 - acc: 0.8146\n",
      "Epoch 127/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3905 - acc: 0.8160\n",
      "Epoch 128/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3927 - acc: 0.8151\n",
      "Epoch 129/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3899 - acc: 0.8163\n",
      "Epoch 130/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3880 - acc: 0.8180\n",
      "Epoch 131/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3892 - acc: 0.8171\n",
      "Epoch 132/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3876 - acc: 0.8188\n",
      "Epoch 133/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3887 - acc: 0.8177\n",
      "Epoch 134/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3858 - acc: 0.8205\n",
      "Epoch 135/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3866 - acc: 0.8188\n",
      "Epoch 136/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3855 - acc: 0.8195\n",
      "Epoch 137/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3849 - acc: 0.8191\n",
      "Epoch 138/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3814 - acc: 0.8226\n",
      "Epoch 139/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3861 - acc: 0.8192\n",
      "Epoch 140/500\n",
      "96000/96000 [==============================] - 19s 194us/step - loss: 0.3832 - acc: 0.8203\n",
      "Epoch 141/500\n",
      "96000/96000 [==============================] - 19s 194us/step - loss: 0.3830 - acc: 0.8222\n",
      "Epoch 142/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3816 - acc: 0.8232\n",
      "Epoch 143/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3805 - acc: 0.8220\n",
      "Epoch 144/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3818 - acc: 0.8212\n",
      "Epoch 145/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3794 - acc: 0.8233\n",
      "Epoch 146/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3790 - acc: 0.8234\n",
      "Epoch 147/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3798 - acc: 0.8225\n",
      "Epoch 148/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3777 - acc: 0.8239\n",
      "Epoch 149/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3762 - acc: 0.8248\n",
      "Epoch 150/500\n",
      "96000/96000 [==============================] - 18s 193us/step - loss: 0.3778 - acc: 0.8240\n",
      "Epoch 151/500\n",
      "96000/96000 [==============================] - 18s 193us/step - loss: 0.3766 - acc: 0.8234\n",
      "Epoch 152/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3766 - acc: 0.8239\n",
      "Epoch 153/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3737 - acc: 0.8260\n",
      "Epoch 154/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3761 - acc: 0.8255\n",
      "Epoch 155/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3740 - acc: 0.8259\n",
      "Epoch 156/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3746 - acc: 0.8255\n",
      "Epoch 157/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3732 - acc: 0.8278\n",
      "Epoch 158/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3743 - acc: 0.8247\n",
      "Epoch 159/500\n",
      "96000/96000 [==============================] - 18s 193us/step - loss: 0.3725 - acc: 0.8279\n",
      "Epoch 160/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3719 - acc: 0.8275\n",
      "Epoch 161/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3727 - acc: 0.8276\n",
      "Epoch 162/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3711 - acc: 0.8277\n",
      "Epoch 163/500\n",
      "96000/96000 [==============================] - 19s 194us/step - loss: 0.3697 - acc: 0.8293\n",
      "Epoch 164/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3701 - acc: 0.8282\n",
      "Epoch 165/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3702 - acc: 0.8278\n",
      "Epoch 166/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3695 - acc: 0.8293\n",
      "Epoch 167/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3702 - acc: 0.8289\n",
      "Epoch 168/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3669 - acc: 0.8302\n",
      "Epoch 169/500\n",
      "96000/96000 [==============================] - 18s 193us/step - loss: 0.3687 - acc: 0.8295\n",
      "Epoch 170/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3677 - acc: 0.8294\n",
      "Epoch 171/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3653 - acc: 0.8315\n",
      "Epoch 172/500\n",
      "96000/96000 [==============================] - 19s 194us/step - loss: 0.3657 - acc: 0.8318\n",
      "Epoch 173/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3654 - acc: 0.8312\n",
      "Epoch 174/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3649 - acc: 0.8316\n",
      "Epoch 175/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3627 - acc: 0.8331\n",
      "Epoch 176/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3649 - acc: 0.8321\n",
      "Epoch 177/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3633 - acc: 0.8330\n",
      "Epoch 178/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3632 - acc: 0.8330\n",
      "Epoch 179/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3617 - acc: 0.8330\n",
      "Epoch 180/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3641 - acc: 0.8318\n",
      "Epoch 181/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3625 - acc: 0.8325\n",
      "Epoch 182/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3614 - acc: 0.8333\n",
      "Epoch 183/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3611 - acc: 0.8331\n",
      "Epoch 184/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3603 - acc: 0.8339\n",
      "Epoch 185/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3594 - acc: 0.8356\n",
      "Epoch 186/500\n",
      "96000/96000 [==============================] - 19s 194us/step - loss: 0.3601 - acc: 0.8345\n",
      "Epoch 187/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3585 - acc: 0.8347\n",
      "Epoch 188/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3607 - acc: 0.8343\n",
      "Epoch 189/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3586 - acc: 0.8345\n",
      "Epoch 190/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3586 - acc: 0.8356\n",
      "Epoch 191/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3562 - acc: 0.8367\n",
      "Epoch 192/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3571 - acc: 0.8361\n",
      "Epoch 193/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3581 - acc: 0.8365\n",
      "Epoch 194/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3565 - acc: 0.8364\n",
      "Epoch 195/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3562 - acc: 0.8381\n",
      "Epoch 196/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3558 - acc: 0.8375\n",
      "Epoch 197/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3553 - acc: 0.8369\n",
      "Epoch 198/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3556 - acc: 0.8380\n",
      "Epoch 199/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3526 - acc: 0.8391\n",
      "Epoch 200/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3543 - acc: 0.8385\n",
      "Epoch 201/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3535 - acc: 0.8384\n",
      "Epoch 202/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3550 - acc: 0.8368\n",
      "Epoch 203/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3545 - acc: 0.8371\n",
      "Epoch 204/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3521 - acc: 0.8387\n",
      "Epoch 205/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3534 - acc: 0.8373\n",
      "Epoch 206/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3503 - acc: 0.8412\n",
      "Epoch 207/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3522 - acc: 0.8387\n",
      "Epoch 208/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3515 - acc: 0.8383\n",
      "Epoch 209/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3517 - acc: 0.8393\n",
      "Epoch 210/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3510 - acc: 0.8395\n",
      "Epoch 211/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3507 - acc: 0.8401\n",
      "Epoch 212/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3494 - acc: 0.8408\n",
      "Epoch 213/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3484 - acc: 0.8417\n",
      "Epoch 214/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3498 - acc: 0.8403\n",
      "Epoch 215/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3480 - acc: 0.8423\n",
      "Epoch 216/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3481 - acc: 0.8412\n",
      "Epoch 217/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3477 - acc: 0.8415\n",
      "Epoch 218/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3480 - acc: 0.8420\n",
      "Epoch 219/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3478 - acc: 0.8422\n",
      "Epoch 220/500\n",
      "96000/96000 [==============================] - 18s 193us/step - loss: 0.3457 - acc: 0.8430\n",
      "Epoch 221/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3468 - acc: 0.8422\n",
      "Epoch 222/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3445 - acc: 0.8431\n",
      "Epoch 223/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3470 - acc: 0.8414\n",
      "Epoch 224/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3451 - acc: 0.8421\n",
      "Epoch 225/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3471 - acc: 0.8429\n",
      "Epoch 226/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3444 - acc: 0.8438\n",
      "Epoch 227/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3436 - acc: 0.8437\n",
      "Epoch 228/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3449 - acc: 0.8427\n",
      "Epoch 229/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3433 - acc: 0.8444\n",
      "Epoch 230/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3428 - acc: 0.8431\n",
      "Epoch 231/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3436 - acc: 0.8441\n",
      "Epoch 232/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3422 - acc: 0.8443\n",
      "Epoch 233/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3436 - acc: 0.8431\n",
      "Epoch 234/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3433 - acc: 0.8439\n",
      "Epoch 235/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3405 - acc: 0.8452\n",
      "Epoch 236/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3417 - acc: 0.8454\n",
      "Epoch 237/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3410 - acc: 0.8451\n",
      "Epoch 238/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3395 - acc: 0.8446\n",
      "Epoch 239/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3398 - acc: 0.8462\n",
      "Epoch 240/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3401 - acc: 0.8461\n",
      "Epoch 241/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3403 - acc: 0.8461\n",
      "Epoch 242/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3393 - acc: 0.8451\n",
      "Epoch 243/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3393 - acc: 0.8461\n",
      "Epoch 244/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3389 - acc: 0.8457\n",
      "Epoch 245/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3385 - acc: 0.8459\n",
      "Epoch 246/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3391 - acc: 0.8468\n",
      "Epoch 247/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3374 - acc: 0.8467\n",
      "Epoch 248/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3389 - acc: 0.8470\n",
      "Epoch 249/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3373 - acc: 0.8464\n",
      "Epoch 250/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3355 - acc: 0.8474\n",
      "Epoch 251/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3368 - acc: 0.8478\n",
      "Epoch 252/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3382 - acc: 0.8472\n",
      "Epoch 253/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3350 - acc: 0.8491\n",
      "Epoch 254/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3348 - acc: 0.8498\n",
      "Epoch 255/500\n",
      "96000/96000 [==============================] - 19s 195us/step - loss: 0.3364 - acc: 0.8477\n",
      "Epoch 256/500\n",
      "96000/96000 [==============================] - 19s 195us/step - loss: 0.3351 - acc: 0.8490\n",
      "Epoch 257/500\n",
      "96000/96000 [==============================] - 19s 194us/step - loss: 0.3349 - acc: 0.8476\n",
      "Epoch 258/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3342 - acc: 0.8479\n",
      "Epoch 259/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3343 - acc: 0.8493\n",
      "Epoch 260/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3344 - acc: 0.8488\n",
      "Epoch 261/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3341 - acc: 0.8499\n",
      "Epoch 262/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3324 - acc: 0.8496\n",
      "Epoch 263/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3341 - acc: 0.8493\n",
      "Epoch 264/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3343 - acc: 0.8487\n",
      "Epoch 265/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3340 - acc: 0.8492\n",
      "Epoch 266/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3316 - acc: 0.8513\n",
      "Epoch 267/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3331 - acc: 0.8502\n",
      "Epoch 268/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3328 - acc: 0.8496\n",
      "Epoch 269/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3325 - acc: 0.8501\n",
      "Epoch 270/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3301 - acc: 0.8518\n",
      "Epoch 271/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3327 - acc: 0.8498\n",
      "Epoch 272/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3331 - acc: 0.8497\n",
      "Epoch 273/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3303 - acc: 0.8511\n",
      "Epoch 274/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3299 - acc: 0.8518\n",
      "Epoch 275/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3309 - acc: 0.8499\n",
      "Epoch 276/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3298 - acc: 0.8514\n",
      "Epoch 277/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3297 - acc: 0.8507\n",
      "Epoch 278/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3300 - acc: 0.8510\n",
      "Epoch 279/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3291 - acc: 0.8510\n",
      "Epoch 280/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3285 - acc: 0.8524\n",
      "Epoch 281/500\n",
      "96000/96000 [==============================] - 18s 193us/step - loss: 0.3298 - acc: 0.8508\n",
      "Epoch 282/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3277 - acc: 0.8522\n",
      "Epoch 283/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3283 - acc: 0.8526\n",
      "Epoch 284/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3291 - acc: 0.8519\n",
      "Epoch 285/500\n",
      "96000/96000 [==============================] - 20s 206us/step - loss: 0.3273 - acc: 0.8526\n",
      "Epoch 286/500\n",
      "96000/96000 [==============================] - 24s 248us/step - loss: 0.3276 - acc: 0.8529\n",
      "Epoch 287/500\n",
      "96000/96000 [==============================] - 20s 205us/step - loss: 0.3270 - acc: 0.8543\n",
      "Epoch 288/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3276 - acc: 0.8524\n",
      "Epoch 289/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3259 - acc: 0.8539\n",
      "Epoch 290/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3260 - acc: 0.8535\n",
      "Epoch 291/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3261 - acc: 0.8532\n",
      "Epoch 292/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3261 - acc: 0.8531\n",
      "Epoch 293/500\n",
      "96000/96000 [==============================] - 19s 196us/step - loss: 0.3241 - acc: 0.8543\n",
      "Epoch 294/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3251 - acc: 0.8540\n",
      "Epoch 295/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3252 - acc: 0.8537\n",
      "Epoch 296/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3244 - acc: 0.8542\n",
      "Epoch 297/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3254 - acc: 0.8536\n",
      "Epoch 298/500\n",
      "96000/96000 [==============================] - 18s 193us/step - loss: 0.3240 - acc: 0.8550\n",
      "Epoch 299/500\n",
      "96000/96000 [==============================] - 19s 194us/step - loss: 0.3247 - acc: 0.8545\n",
      "Epoch 300/500\n",
      "96000/96000 [==============================] - 18s 193us/step - loss: 0.3227 - acc: 0.8553\n",
      "Epoch 301/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3252 - acc: 0.8544\n",
      "Epoch 302/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3230 - acc: 0.8541\n",
      "Epoch 303/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3227 - acc: 0.8556\n",
      "Epoch 304/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3234 - acc: 0.8544\n",
      "Epoch 305/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3223 - acc: 0.8549\n",
      "Epoch 306/500\n",
      "96000/96000 [==============================] - 19s 194us/step - loss: 0.3228 - acc: 0.8557\n",
      "Epoch 307/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3228 - acc: 0.8558\n",
      "Epoch 308/500\n",
      "96000/96000 [==============================] - 19s 197us/step - loss: 0.3208 - acc: 0.8560\n",
      "Epoch 309/500\n",
      "96000/96000 [==============================] - 19s 194us/step - loss: 0.3226 - acc: 0.8556\n",
      "Epoch 310/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3212 - acc: 0.8556\n",
      "Epoch 311/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3215 - acc: 0.8558\n",
      "Epoch 312/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3211 - acc: 0.8565\n",
      "Epoch 313/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96000/96000 [==============================] - 19s 194us/step - loss: 0.3195 - acc: 0.8571\n",
      "Epoch 314/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3212 - acc: 0.8569\n",
      "Epoch 315/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3196 - acc: 0.8567\n",
      "Epoch 316/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3207 - acc: 0.8569\n",
      "Epoch 317/500\n",
      "96000/96000 [==============================] - 18s 193us/step - loss: 0.3200 - acc: 0.8572\n",
      "Epoch 318/500\n",
      "96000/96000 [==============================] - 19s 194us/step - loss: 0.3206 - acc: 0.8560\n",
      "Epoch 319/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3187 - acc: 0.8571\n",
      "Epoch 320/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3208 - acc: 0.8565\n",
      "Epoch 321/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3192 - acc: 0.8581\n",
      "Epoch 322/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3172 - acc: 0.8587\n",
      "Epoch 323/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3189 - acc: 0.8579\n",
      "Epoch 324/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3177 - acc: 0.8586\n",
      "Epoch 325/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3178 - acc: 0.8571\n",
      "Epoch 326/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3195 - acc: 0.8577\n",
      "Epoch 327/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3178 - acc: 0.8589\n",
      "Epoch 328/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3173 - acc: 0.8580\n",
      "Epoch 329/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3191 - acc: 0.8572\n",
      "Epoch 330/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3185 - acc: 0.8588\n",
      "Epoch 331/500\n",
      "96000/96000 [==============================] - 18s 193us/step - loss: 0.3160 - acc: 0.8597\n",
      "Epoch 332/500\n",
      "96000/96000 [==============================] - 18s 193us/step - loss: 0.3161 - acc: 0.8592\n",
      "Epoch 333/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3158 - acc: 0.8593\n",
      "Epoch 334/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3162 - acc: 0.8589\n",
      "Epoch 335/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3152 - acc: 0.8599\n",
      "Epoch 336/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3170 - acc: 0.8586\n",
      "Epoch 337/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3150 - acc: 0.8593\n",
      "Epoch 338/500\n",
      "96000/96000 [==============================] - 19s 194us/step - loss: 0.3156 - acc: 0.8592\n",
      "Epoch 339/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3155 - acc: 0.8581\n",
      "Epoch 340/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3171 - acc: 0.8589\n",
      "Epoch 341/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3138 - acc: 0.8601\n",
      "Epoch 342/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3161 - acc: 0.8589\n",
      "Epoch 343/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3144 - acc: 0.8599\n",
      "Epoch 344/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3152 - acc: 0.8587\n",
      "Epoch 345/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3126 - acc: 0.8606\n",
      "Epoch 346/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3147 - acc: 0.8595\n",
      "Epoch 347/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3147 - acc: 0.8595\n",
      "Epoch 348/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3141 - acc: 0.8605\n",
      "Epoch 349/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3126 - acc: 0.8606\n",
      "Epoch 350/500\n",
      "96000/96000 [==============================] - 18s 193us/step - loss: 0.3118 - acc: 0.8607\n",
      "Epoch 351/500\n",
      "96000/96000 [==============================] - 18s 193us/step - loss: 0.3112 - acc: 0.8610\n",
      "Epoch 352/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3117 - acc: 0.8615\n",
      "Epoch 353/500\n",
      "96000/96000 [==============================] - 19s 198us/step - loss: 0.3108 - acc: 0.8609\n",
      "Epoch 354/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3125 - acc: 0.8607\n",
      "Epoch 355/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3105 - acc: 0.8614\n",
      "Epoch 356/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3115 - acc: 0.8614\n",
      "Epoch 357/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3126 - acc: 0.8610\n",
      "Epoch 358/500\n",
      "96000/96000 [==============================] - 19s 194us/step - loss: 0.3115 - acc: 0.8611\n",
      "Epoch 359/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3117 - acc: 0.8598\n",
      "Epoch 360/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3114 - acc: 0.8623\n",
      "Epoch 361/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3099 - acc: 0.8620\n",
      "Epoch 362/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3116 - acc: 0.8609\n",
      "Epoch 363/500\n",
      "96000/96000 [==============================] - 18s 193us/step - loss: 0.3106 - acc: 0.8616\n",
      "Epoch 364/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3104 - acc: 0.8621\n",
      "Epoch 365/500\n",
      "96000/96000 [==============================] - 18s 193us/step - loss: 0.3099 - acc: 0.8622\n",
      "Epoch 366/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3104 - acc: 0.8627\n",
      "Epoch 367/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3096 - acc: 0.8620\n",
      "Epoch 368/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3097 - acc: 0.8628\n",
      "Epoch 369/500\n",
      "96000/96000 [==============================] - 19s 198us/step - loss: 0.3089 - acc: 0.8628\n",
      "Epoch 370/500\n",
      "96000/96000 [==============================] - 19s 197us/step - loss: 0.3093 - acc: 0.8632\n",
      "Epoch 371/500\n",
      "96000/96000 [==============================] - 19s 195us/step - loss: 0.3087 - acc: 0.8626\n",
      "Epoch 372/500\n",
      "96000/96000 [==============================] - 18s 193us/step - loss: 0.3081 - acc: 0.8633\n",
      "Epoch 373/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3080 - acc: 0.8634\n",
      "Epoch 374/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3080 - acc: 0.8632\n",
      "Epoch 375/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3089 - acc: 0.8625\n",
      "Epoch 376/500\n",
      "96000/96000 [==============================] - 18s 193us/step - loss: 0.3075 - acc: 0.8628\n",
      "Epoch 377/500\n",
      "96000/96000 [==============================] - 18s 193us/step - loss: 0.3063 - acc: 0.8634\n",
      "Epoch 378/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3077 - acc: 0.8631\n",
      "Epoch 379/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3081 - acc: 0.8636\n",
      "Epoch 380/500\n",
      "96000/96000 [==============================] - 19s 198us/step - loss: 0.3069 - acc: 0.8636\n",
      "Epoch 381/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3071 - acc: 0.8632\n",
      "Epoch 382/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3069 - acc: 0.8634\n",
      "Epoch 383/500\n",
      "96000/96000 [==============================] - 19s 194us/step - loss: 0.3059 - acc: 0.8639\n",
      "Epoch 384/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3055 - acc: 0.8639\n",
      "Epoch 385/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3064 - acc: 0.8634\n",
      "Epoch 386/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3066 - acc: 0.8635\n",
      "Epoch 387/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3047 - acc: 0.8645\n",
      "Epoch 388/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3069 - acc: 0.8642\n",
      "Epoch 389/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3033 - acc: 0.8658\n",
      "Epoch 390/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3041 - acc: 0.8648\n",
      "Epoch 391/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3064 - acc: 0.8631\n",
      "Epoch 392/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3037 - acc: 0.8647\n",
      "Epoch 393/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3048 - acc: 0.8646\n",
      "Epoch 394/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3051 - acc: 0.8640\n",
      "Epoch 395/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3040 - acc: 0.8653\n",
      "Epoch 396/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3040 - acc: 0.8650\n",
      "Epoch 397/500\n",
      "96000/96000 [==============================] - 19s 194us/step - loss: 0.3038 - acc: 0.8664\n",
      "Epoch 398/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3041 - acc: 0.8648\n",
      "Epoch 399/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3034 - acc: 0.8658\n",
      "Epoch 400/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3025 - acc: 0.8668\n",
      "Epoch 401/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3022 - acc: 0.8662\n",
      "Epoch 402/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3034 - acc: 0.8645\n",
      "Epoch 403/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3027 - acc: 0.8656\n",
      "Epoch 404/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3022 - acc: 0.8666\n",
      "Epoch 405/500\n",
      "96000/96000 [==============================] - 18s 193us/step - loss: 0.3033 - acc: 0.8644\n",
      "Epoch 406/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3019 - acc: 0.8649\n",
      "Epoch 407/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3028 - acc: 0.8654\n",
      "Epoch 408/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3018 - acc: 0.8656\n",
      "Epoch 409/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3018 - acc: 0.8666\n",
      "Epoch 410/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3041 - acc: 0.8650\n",
      "Epoch 411/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3016 - acc: 0.8662\n",
      "Epoch 412/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3006 - acc: 0.8664\n",
      "Epoch 413/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.3019 - acc: 0.8658\n",
      "Epoch 414/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3004 - acc: 0.8674\n",
      "Epoch 415/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.3024 - acc: 0.8662\n",
      "Epoch 416/500\n",
      "96000/96000 [==============================] - 19s 195us/step - loss: 0.3017 - acc: 0.8661\n",
      "Epoch 417/500\n",
      "96000/96000 [==============================] - 19s 197us/step - loss: 0.3009 - acc: 0.8666\n",
      "Epoch 418/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3009 - acc: 0.8661\n",
      "Epoch 419/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3014 - acc: 0.8662\n",
      "Epoch 420/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.3000 - acc: 0.8669\n",
      "Epoch 421/500\n",
      "96000/96000 [==============================] - 20s 208us/step - loss: 0.2985 - acc: 0.8675\n",
      "Epoch 422/500\n",
      "96000/96000 [==============================] - 20s 211us/step - loss: 0.3004 - acc: 0.8674\n",
      "Epoch 423/500\n",
      "96000/96000 [==============================] - 21s 217us/step - loss: 0.2997 - acc: 0.8669\n",
      "Epoch 424/500\n",
      "96000/96000 [==============================] - 23s 244us/step - loss: 0.2988 - acc: 0.8671\n",
      "Epoch 425/500\n",
      "96000/96000 [==============================] - 19s 197us/step - loss: 0.2986 - acc: 0.8679\n",
      "Epoch 426/500\n",
      "96000/96000 [==============================] - 19s 201us/step - loss: 0.2981 - acc: 0.8687\n",
      "Epoch 427/500\n",
      "96000/96000 [==============================] - 20s 203us/step - loss: 0.2976 - acc: 0.8687\n",
      "Epoch 428/500\n",
      "96000/96000 [==============================] - 20s 204us/step - loss: 0.2999 - acc: 0.8668\n",
      "Epoch 429/500\n",
      "96000/96000 [==============================] - 19s 200us/step - loss: 0.2970 - acc: 0.8686\n",
      "Epoch 430/500\n",
      "96000/96000 [==============================] - 19s 200us/step - loss: 0.2986 - acc: 0.8673\n",
      "Epoch 431/500\n",
      "96000/96000 [==============================] - 19s 198us/step - loss: 0.2967 - acc: 0.8692\n",
      "Epoch 432/500\n",
      "96000/96000 [==============================] - 19s 200us/step - loss: 0.2988 - acc: 0.8686\n",
      "Epoch 433/500\n",
      "96000/96000 [==============================] - 19s 198us/step - loss: 0.2990 - acc: 0.8676\n",
      "Epoch 434/500\n",
      "96000/96000 [==============================] - 19s 201us/step - loss: 0.2972 - acc: 0.8681\n",
      "Epoch 435/500\n",
      "96000/96000 [==============================] - 19s 195us/step - loss: 0.2960 - acc: 0.8694\n",
      "Epoch 436/500\n",
      "96000/96000 [==============================] - 19s 199us/step - loss: 0.2957 - acc: 0.8687\n",
      "Epoch 437/500\n",
      "96000/96000 [==============================] - 19s 196us/step - loss: 0.2972 - acc: 0.8688\n",
      "Epoch 438/500\n",
      "96000/96000 [==============================] - 19s 201us/step - loss: 0.2979 - acc: 0.8679\n",
      "Epoch 439/500\n",
      "96000/96000 [==============================] - 19s 196us/step - loss: 0.2968 - acc: 0.8682\n",
      "Epoch 440/500\n",
      "96000/96000 [==============================] - 19s 200us/step - loss: 0.2960 - acc: 0.8692\n",
      "Epoch 441/500\n",
      "96000/96000 [==============================] - 19s 200us/step - loss: 0.2968 - acc: 0.8677\n",
      "Epoch 442/500\n",
      "96000/96000 [==============================] - 19s 197us/step - loss: 0.2975 - acc: 0.8693\n",
      "Epoch 443/500\n",
      "96000/96000 [==============================] - 20s 203us/step - loss: 0.2949 - acc: 0.8694\n",
      "Epoch 444/500\n",
      "96000/96000 [==============================] - 19s 196us/step - loss: 0.2977 - acc: 0.8690\n",
      "Epoch 445/500\n",
      "96000/96000 [==============================] - 19s 201us/step - loss: 0.2949 - acc: 0.8692\n",
      "Epoch 446/500\n",
      "96000/96000 [==============================] - 19s 202us/step - loss: 0.2957 - acc: 0.8687\n",
      "Epoch 447/500\n",
      "96000/96000 [==============================] - 19s 202us/step - loss: 0.2951 - acc: 0.8693\n",
      "Epoch 448/500\n",
      "96000/96000 [==============================] - 19s 199us/step - loss: 0.2957 - acc: 0.8696\n",
      "Epoch 449/500\n",
      "96000/96000 [==============================] - 19s 199us/step - loss: 0.2936 - acc: 0.8705\n",
      "Epoch 450/500\n",
      "96000/96000 [==============================] - 19s 201us/step - loss: 0.2948 - acc: 0.8697\n",
      "Epoch 451/500\n",
      "96000/96000 [==============================] - 19s 199us/step - loss: 0.2953 - acc: 0.8700\n",
      "Epoch 452/500\n",
      "96000/96000 [==============================] - 19s 198us/step - loss: 0.2955 - acc: 0.8696\n",
      "Epoch 453/500\n",
      "96000/96000 [==============================] - 19s 199us/step - loss: 0.2926 - acc: 0.8712\n",
      "Epoch 454/500\n",
      "96000/96000 [==============================] - 19s 199us/step - loss: 0.2938 - acc: 0.8692\n",
      "Epoch 455/500\n",
      "96000/96000 [==============================] - 19s 196us/step - loss: 0.2948 - acc: 0.8699\n",
      "Epoch 456/500\n",
      "96000/96000 [==============================] - 19s 198us/step - loss: 0.2945 - acc: 0.8699\n",
      "Epoch 457/500\n",
      "96000/96000 [==============================] - 19s 198us/step - loss: 0.2945 - acc: 0.8697\n",
      "Epoch 458/500\n",
      "96000/96000 [==============================] - 19s 199us/step - loss: 0.2932 - acc: 0.8707\n",
      "Epoch 459/500\n",
      "96000/96000 [==============================] - 19s 199us/step - loss: 0.2940 - acc: 0.8700\n",
      "Epoch 460/500\n",
      "96000/96000 [==============================] - 19s 200us/step - loss: 0.2921 - acc: 0.8717\n",
      "Epoch 461/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.2923 - acc: 0.8700\n",
      "Epoch 462/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.2925 - acc: 0.8715\n",
      "Epoch 463/500\n",
      "96000/96000 [==============================] - 19s 199us/step - loss: 0.2908 - acc: 0.8717\n",
      "Epoch 464/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.2924 - acc: 0.8712\n",
      "Epoch 465/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.2920 - acc: 0.8704\n",
      "Epoch 466/500\n",
      "96000/96000 [==============================] - 18s 193us/step - loss: 0.2928 - acc: 0.8705\n",
      "Epoch 467/500\n",
      "96000/96000 [==============================] - 19s 197us/step - loss: 0.2917 - acc: 0.8712\n",
      "Epoch 468/500\n",
      "96000/96000 [==============================] - 19s 197us/step - loss: 0.2918 - acc: 0.8710\n",
      "Epoch 469/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96000/96000 [==============================] - 19s 194us/step - loss: 0.2933 - acc: 0.8704\n",
      "Epoch 470/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.2917 - acc: 0.8718\n",
      "Epoch 471/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.2921 - acc: 0.8708\n",
      "Epoch 472/500\n",
      "96000/96000 [==============================] - 18s 190us/step - loss: 0.2901 - acc: 0.8724\n",
      "Epoch 473/500\n",
      "96000/96000 [==============================] - 19s 195us/step - loss: 0.2905 - acc: 0.8720\n",
      "Epoch 474/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.2892 - acc: 0.8724\n",
      "Epoch 475/500\n",
      "96000/96000 [==============================] - 18s 190us/step - loss: 0.2907 - acc: 0.8717\n",
      "Epoch 476/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.2888 - acc: 0.8727\n",
      "Epoch 477/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.2911 - acc: 0.8718\n",
      "Epoch 478/500\n",
      "96000/96000 [==============================] - 18s 190us/step - loss: 0.2894 - acc: 0.8719\n",
      "Epoch 479/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.2897 - acc: 0.8725\n",
      "Epoch 480/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.2905 - acc: 0.8714\n",
      "Epoch 481/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.2894 - acc: 0.8726\n",
      "Epoch 482/500\n",
      "96000/96000 [==============================] - 18s 193us/step - loss: 0.2889 - acc: 0.8729\n",
      "Epoch 483/500\n",
      "96000/96000 [==============================] - 18s 190us/step - loss: 0.2902 - acc: 0.8724\n",
      "Epoch 484/500\n",
      "96000/96000 [==============================] - 18s 190us/step - loss: 0.2896 - acc: 0.8728\n",
      "Epoch 485/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.2892 - acc: 0.8723\n",
      "Epoch 486/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.2889 - acc: 0.8730\n",
      "Epoch 487/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.2878 - acc: 0.8740\n",
      "Epoch 488/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.2890 - acc: 0.8736\n",
      "Epoch 489/500\n",
      "96000/96000 [==============================] - 18s 193us/step - loss: 0.2888 - acc: 0.8725\n",
      "Epoch 490/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.2882 - acc: 0.8735\n",
      "Epoch 491/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.2890 - acc: 0.8734\n",
      "Epoch 492/500\n",
      "96000/96000 [==============================] - 19s 193us/step - loss: 0.2891 - acc: 0.8728\n",
      "Epoch 493/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.2859 - acc: 0.8738\n",
      "Epoch 494/500\n",
      "96000/96000 [==============================] - 18s 190us/step - loss: 0.2882 - acc: 0.8724\n",
      "Epoch 495/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.2884 - acc: 0.8728\n",
      "Epoch 496/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.2879 - acc: 0.8735\n",
      "Epoch 497/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.2870 - acc: 0.8741\n",
      "Epoch 498/500\n",
      "96000/96000 [==============================] - 18s 191us/step - loss: 0.2871 - acc: 0.8737\n",
      "Epoch 499/500\n",
      "96000/96000 [==============================] - 18s 192us/step - loss: 0.2864 - acc: 0.8734\n",
      "Epoch 500/500\n",
      "96000/96000 [==============================] - 18s 190us/step - loss: 0.2878 - acc: 0.8734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x153b71d29b0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(hotEmbedFeatures,trainLabels,epochs=500,batch_size=4000,shuffle=True,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "96000/96000 [==============================] - 6s 64us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.25946868769824505, 0.8865312511722246]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.metrics_names)\n",
    "model.evaluate(hotEmbedFeatures,trainLabels,batch_size=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000\n",
      "24000\n",
      "24000\n",
      "24000\n",
      "20000\n",
      "prp he vbd dt a K nn innn on dt the nn end end end end end end end end end end end end\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "equalLenTestSet,testLabels = makeSentEqLen(testSet,seqLen)\n",
    "print(len(equalLenTestSet))\n",
    "testLabels = np.array(testLabels)\n",
    "print(len(testSet))\n",
    "print(len(equalLenTestSet))\n",
    "print(len(testLabels))\n",
    "equalLenTestSet = np.append(equalLenTestSet[0:19998],equalLenTestSet[0:2],axis=0)\n",
    "print(len(equalLenTestSet))\n",
    "print(equalLenTestSet[0])\n",
    "testLabels = np.append(testLabels[0:19998],testLabels[0:2],axis=0)\n",
    "print(len(testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 24, 75)\n"
     ]
    }
   ],
   "source": [
    "testhotEmbedFeatures = []\n",
    "sentCount = 0\n",
    "embedLength = 75\n",
    "for sent in equalLenTestSet:\n",
    "    \n",
    "    sentvector = []\n",
    "    \n",
    "    for word in sent.split():\n",
    "       \n",
    "        wordVec = np.zeros(embedLength)\n",
    "        if word in posTag_int:\n",
    "            #print(posTag_int['a'])\n",
    "            #print(word)\n",
    "            #print(postTag_int['a'])\n",
    "            \n",
    "            wordVec[posTag_int[word]]=1\n",
    "            #print(wordVec)\n",
    "        elif(word in word2vec_model ):\n",
    "            wordVec = word2vec_model.word_vec(word)[:embedLength]\n",
    "        elif(word==\"end\"):\n",
    "            wordVec[1]=1\n",
    "        sentvector = np.append(sentvector,wordVec)\n",
    "    \n",
    "    testhotEmbedFeatures.append(sentvector)\n",
    "    #print(sent)\n",
    "    sentCount = sentCount+1 \n",
    "\n",
    "testhotEmbedFeatures = np.array(testhotEmbedFeatures)  \n",
    "\n",
    "testhotEmbedFeatures = np.reshape(testhotEmbedFeatures,(len(testhotEmbedFeatures),seqLen,embedLength))\n",
    "print(testhotEmbedFeatures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "20000/20000 [==============================] - 1s 65us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3238819122314453, 0.8675000071525574]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.metrics_names)\n",
    "model.evaluate(testhotEmbedFeatures,testLabels,batch_size=4000)\n",
    "#print(testLabels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
