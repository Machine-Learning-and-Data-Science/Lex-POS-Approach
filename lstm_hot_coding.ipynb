{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99685\n",
      "prp i rb vbz vbn prpd my jj nns\n",
      "jj nn innn on prpd your jj nn\n"
     ]
    }
   ],
   "source": [
   "correctSentFile = \"C:\\\\Python364\\\\Scripts\\\\Grammar Corrector\\\\combined_tagged_lang_selected_entries.csv\"\n",
    "corrSent = pd.read_csv(correctSentFile,header=None,sep=\"\\t\")\n",
    "\n",
    "#corrSent = corrSent[4]\n",
    "corrSent = corrSent[2]\n",
    "tempSent = []\n",
    "print(len(corrSent))\n",
    "for sent in corrSent:\n",
    "    sent = sent.strip()\n",
    "    sent = re.sub(r'\\.{1,}',r'',sent)\n",
    "    tempSent.append(sent)\n",
    "'''\n",
    "shuffledIndexFile = (\"C:\\\\Python364\\\\Scripts\\\\Grammar Corrector\\\\shuffled-index-correct_sent.csv\")\n",
    "shuffledIndex = pd.read_csv(shuffledIndexFile,header=None,sep=\"\\t\")\n",
    "shuffledIndex = shuffledIndex[0]\n",
    "'''\n",
    "bl = list(range(len(corrSent)))\n",
    "\n",
    "random.shuffle(bl)\n",
    "corrSent = []\n",
    "for j in bl :\n",
    "    corrSent.append(tempSent[j])\n",
    "    \n",
    "print(corrSent[0])\n",
    "print(tempSent[0])\n",
    "#corrSent[110:115]\n",
    "#corrSent = ['a b c d e','a b c d e','a b c d e','a b c d e','a b c d e','a b c d e','a b c d e','a b c d e','a b c d e','a b c d e']\n",
    "#print(corrSent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31623\n",
      "prp i vbp fouuund dt a Y jj nn innn for vbg jj nn\n",
      "jj nn innn at prpd your jj nn\n",
      "92082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nreversed_arr = []\\nfor i in range(len(corrSent)):\\n    reversed_arr.append(corrSent[i][::-1])\\nprint(reversed_arr)\\nIncorrSent = reversed_arr\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inCorrectSentFile = \"C:\\\\Python364\\\\Scripts\\\\Grammar Corrector\\\\5-combined-tagged-words-wrong-sent1.csv\"\n",
    "#inCorrectSentFile = \"C:\\\\Python364\\\\Scripts\\\\Grammar Corrector\\\\combined-tagged-incorrect-sent2.csv\"\n",
    "#inCorrectSentFile = \"C:\\\\Python364\\\\Scripts\\\\Grammar Corrector\\\\combined-tagged-wrong-sent-2(general errors)2.csv\"\n",
    "inCorrectSentFile = \"C:\\\\Python364\\\\Scripts\\\\Grammar Corrector\\\\combined_tagged_specific_error_lang_selected_entries.csv\"\n",
    "IncorrSent = pd.read_csv(inCorrectSentFile,header=None,sep=\"\\t\")\n",
    "#IncorrSent = IncorrSent[0]\n",
    "IncorrSent = IncorrSent[2]\n",
    "tempSent = []\n",
    "\n",
    "for sent in IncorrSent:\n",
    "    sent = sent.strip()\n",
    "    sent = re.sub(r'\\.{1,}',r'',sent)\n",
    "    tempSent.append(sent)\n",
    "    \n",
    "print(len(tempSent))\n",
    "    \n",
    "inCorrectSentFile = \"C:\\\\Python364\\\\Scripts\\\\Grammar Corrector\\\\combined_tagged_general_error_lang_selected_entries.csv\"\n",
    "IncorrSent = pd.read_csv(inCorrectSentFile,header=None,sep=\"\\t\")\n",
    "#IncorrSent = IncorrSent[0]\n",
    "IncorrSent = IncorrSent[2]\n",
    "\n",
    "for sent in IncorrSent:\n",
    "    sent = sent.strip()\n",
    "    sent = re.sub(r'\\.{1,}',r'',sent)\n",
    "    tempSent.append(sent)\n",
    "    \n",
    "\n",
    "'''\n",
    "shuffledIndexFile = (\"C:\\\\Python364\\\\Scripts\\\\Grammar Corrector\\\\shuffled-index-incorrect_sent.csv\")\n",
    "shuffledIndex = pd.read_csv(shuffledIndexFile,header=None,sep=\"\\t\")\n",
    "shuffledIndex = shuffledIndex[0] \n",
    "'''\n",
    "bl = list(range(len(tempSent)))\n",
    "random.shuffle(bl)\n",
    "\n",
    "IncorrSent = []\n",
    "for j in bl :\n",
    "    IncorrSent.append(tempSent[j])\n",
    "    \n",
    "print(IncorrSent[0])\n",
    "print(tempSent[0])\n",
    "print(len(IncorrSent))\n",
    "\"\"\"\n",
    "IncorrSent = tempSent\n",
    "print(len(IncorrSent))\n",
    "IncorrSent[0:5]\n",
    "\"\"\"\n",
    "#IncorrSent = ['d e f','d e f','d e f','d e f','d e f','d e f','d e f','d e f','d e f']\n",
    "#IncorrSent = ['b a c','b a c','b a c','b a c','b a c','b a c','b a c','b a c','b a c']\n",
    "\"\"\"\n",
    "reversed_arr = []\n",
    "for i in range(len(corrSent)):\n",
    "    reversed_arr.append(corrSent[i][::-1])\n",
    "print(reversed_arr)\n",
    "IncorrSent = reversed_arr\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nn vbz dt a V rb jj nn' '0']\n"
     ]
    }
   ],
   "source": [
    "corrSent = np.array(corrSent)\n",
    "\n",
    "zeros = np.zeros((len(corrSent),1),dtype=int) #creating labels of correct sentences\n",
    "corrSent = np.reshape(corrSent,(len(corrSent),1))\n",
    "corrSent = np.append(corrSent,zeros,axis=1) #\n",
    "print(corrSent[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prp i vbp fouuund dt a Y jj nn innn for vbg jj nn' '1']\n"
     ]
    }
   ],
   "source": [
    "IncorrSent = np.array(IncorrSent)\n",
    "ones = np.ones((len(IncorrSent),1),dtype=int) #creating labels of incorrect sentences\n",
    "IncorrSent = np.reshape(IncorrSent,(len(IncorrSent),1))\n",
    "IncorrSent = np.append(IncorrSent,ones,axis=1)\n",
    "print(IncorrSent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nn vbz dt a V rb jj nn' '0']\n",
      "96000\n",
      "24000\n"
     ]
    }
   ],
   "source": [
    "#trainSet = np.append(corrSent[0:40000,],IncorrSent[0:25000,],axis=0)\n",
    "#testSet = np.append(corrSent[40000:50000,],IncorrSent[25000:30000,],axis=0)\n",
    "'''\n",
    "trainSet = np.append(corrSent[0:50000,],IncorrSent[0:25000,],axis=0)\n",
    "trainSet = np.append(trainSet,IncorrSent[61000:86000,],axis=0)\n",
    "testSet = np.append(corrSent[50000:70000,],IncorrSent[25000:30000,],axis=0)\n",
    "testSet = np.append(testSet,IncorrSent[86000:91000,],axis=0)\n",
    "'''\n",
    "\n",
    "#trainSet = np.append(corrSent[0:48000,],IncorrSent[0:48000,],axis=0)\n",
    "#testSet = np.append(corrSent[48000:60000,],IncorrSent[48000:60000,],axis=0)\n",
    "\n",
    "'''\n",
    "trainSet = np.append(corrSent[0:40000,],IncorrSent[0:25000,],axis=0)\n",
    "trainSet = np.append(trainSet[0:65000,],IncorrSent[10000:25000,],axis=0)\n",
    "testSet = np.append(corrSent[40000:50000,],IncorrSent[25000:31623,],axis=0)\n",
    "testSet = np.append(testSet[0:16623,],IncorrSent[11000:14377,],axis=0)\n",
    "'''\n",
    "\n",
    "\n",
    "trainSet = np.append(corrSent[0:48000,],IncorrSent[0:24000,],axis=0)\n",
    "trainSet = np.append(trainSet[0:72000,],IncorrSent[40000:64000,],axis=0)\n",
    "testSet = np.append(corrSent[48000:60000,],IncorrSent[24000:30000,],axis=0)\n",
    "testSet = np.append(testSet[0:18000,],IncorrSent[64000:70000,],axis=0)\n",
    "\n",
    "print(trainSet[2])\n",
    "#trainSet = trainSet[0:700,]\n",
    "#trainSet = trainSet[0:330,]\n",
    "print(len(trainSet))\n",
    "print(len(testSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "966695 English words.\n",
      "1918 unique English words.\n",
      "[('nn', 103601), ('prp', 81211), ('innn', 57643), ('dt', 55890), ('jj', 52392), ('i', 44834), ('rb', 43523), ('nnp', 37792), ('vbp', 34103), ('vb', 30123)]\n",
      "Counter({'nn': 103601, 'prp': 81211, 'innn': 57643, 'dt': 55890, 'jj': 52392, 'i': 44834, 'rb': 43523, 'nnp': 37792, 'vbp': 34103, 'vb': 30123, 'vbd': 28980, 'vbz': 26901, 'nns': 24108, 'the': 17826, 'to': 17355, 'prpd': 17054, ',': 16396, 'cc': 14507, 'a': 14383, 'it': 12523, 'vbg': 12521, 'my': 12361, 'vbn': 11043, 'in': 9952, 'md': 9423, 'an': 8985, 'of': 7636, 'for': 6371, 'am': 6094, 'you': 5875, 'this': 5773, 'have': 4793, 'are': 4783, 'that': 4391, 'me': 4105, 'wrb': 3816, 'at': 3771, 'we': 3745, 'on': 3737, 'wp': 3266, 'with': 3176, 'cd': 3027, 'about': 2944, 'pos': 2783, 'L': 2533, 'like': 2521, 'he': 2518, 'ex': 2388, 'A': 2227, 'from': 2135, 'they': 2129, 'she': 2027, 'do': 1926, 'rp': 1920, 'S': 1883, 'because': 1599, 'K': 1598, 'want': 1595, 'B': 1569, 'jjr': 1560, 'some': 1528, 'by': 1514, 'think': 1423, 'her': 1326, 'all': 1324, 'them': 1291, 'P': 1242, 'G': 1241, 'jjs': 1191, 'F': 1171, 'as': 1152, 'D': 1125, 'if': 1089, 'after': 1051, 'your': 1023, 'M': 1023, 'I': 961, 'no': 936, 'his': 932, 'N': 892, 'wdt': 870, 'T': 869, 'these': 845, 'our': 837, 'hope': 814, 'W': 797, 'H': 784, 'rbr': 776, 'him': 767, 'R': 713, 'than': 678, 'their': 668, '``': 664, 'V': 645, 'every': 596, 'J': 541, 'any': 535, 'feel': 521, 'before': 501, 'since': 486, 'know': 483, 'so': 473, 'love': 465, 'myself': 461, 'us': 423, 'another': 391, 'uh': 367, 'E': 361, 'need': 360, 'pdt': 329, 'over': 326, 'each': 323, 'go': 298, 'live': 288, 'Y': 286, 'during': 269, 'around': 266, 'C': 264, 'rbs': 260, 'into': 260, 'though': 259, 'ate': 245, 'while': 214, 'until': 209, 'both': 200, 'read': 200, 'without': 196, 'between': 190, 'those': 190, 'wish': 188, 'make': 185, 'through': 184, 'its': 178, 'use': 171, 'nnps': 170, 'get': 165, 'study': 152, 'wan': 142, 'say': 138, 'wonder': 137, 'near': 137, 'work': 135, 'believe': 126, 'out': 119, 'off': 115, 'although': 112, 'guess': 110, 'write': 104, 'take': 101, 'try': 101, 'see': 99, 'ago': 96, 'whether': 94, 'enjoy': 94, 'O': 92, 'under': 91, 'look': 90, 'offf': 87, 'ooof': 86, 'hate': 84, 'come': 83, 'eat': 82, 'mean': 75, 'play': 75, 'watch': 72, 'find': 72, 'among': 71, 'fw': 71, 'miss': 69, 'thhhat': 69, 'drank': 64, 'next': 63, 'recommend': 63, 'below': 62, 'outside': 61, 'iiin': 61, 'aaa': 61, 'speak': 60, 'learn': 60, 'heard': 59, 'aaand': 59, 'listen': 57, 'start': 56, 'give': 56, 'correct': 55, 'become': 55, 'against': 55, 'up': 54, 'talk': 54, 'yourself': 53, 'except': 51, 'understand': 50, 'isss': 50, 'ttthat': 50, 'above': 49, 'seem': 48, 'prefer': 48, 'leave': 47, 'haaad': 47, 'waaas': 47, 'wasss': 45, 'help': 44, 'spend': 43, 'aaabout': 43, 'besides': 42, 'wwwas': 42, 'onnn': 42, 'annnd': 41, 'drink': 40, 'behind': 40, 'appreciate': 40, 'hear': 39, 'anddd': 39, 'worry': 37, 'abooout': 37, 'inside': 36, 'm': 36, 'keep': 36, 'along': 36, 'guys': 36, 'enjoyed': 34, 'themselves': 34, 'plan': 34, 'haddd': 34, 'wenttt': 33, \"''\": 32, 'tell': 31, 'per': 31, 'weeent': 31, 'wwwent': 31, 'fooor': 31, 'call': 30, 'buy': 30, 'within': 30, 'put': 28, 'belong': 28, 'remember': 28, 'slept': 28, 'abbbout': 28, 'aaat': 28, 'iiis': 28, 'sleep': 27, 'respect': 27, 'run': 27, 'throughout': 27, 'agree': 27, 'visit': 27, 'wennnt': 27, 'don': 26, 'expect': 26, 'attt': 26, 'forrr': 26, 'fffor': 26, 'dont': 25, 'ooon': 25, 'itself': 24, 'wear': 24, 'stay': 24, 'bbbut': 24, 'via': 23, 'despite': 22, 'toward': 21, 'felt': 21, 'himself': 21, 'lose': 21, 'rode': 20, 'wereee': 20, 'quit': 19, 'dislike': 19, 'tend': 19, 'decide': 19, 'walk': 19, 'wake': 19, 'choose': 19, 'b': 19, 'ask': 18, 'diddd': 18, 'beeecause': 18, 'unlike': 17, 'regret': 17, 'beside': 17, 'either': 17, 'wiiith': 17, 'gottt': 17, 'abouuut': 17, 'buttt': 17, 'byyy': 17, 'across': 16, 'meet': 16, 'sell': 16, 'cook': 16, 'thhhe': 16, 'forget': 15, 'abouttt': 15, 'withhh': 15, 'die': 14, 'drive': 14, 'herself': 14, 'digit': 14, 'werrre': 14, 'please': 13, 'realize': 13, 'bring': 13, 'teach': 13, 'finish': 13, '$': 13, 'snow': 13, 'pass': 13, 'envy': 13, 'overslept': 13, 'appear': 13, 'alllso': 13, 'thattt': 13, 'send': 12, 'wore': 12, 'hurt': 12, 'ourselves': 12, 'hold': 12, 'attend': 12, 'pray': 12, 'continue': 12, 'consider': 12, 'weeere': 12, 'gooot': 12, 'tooook': 12, 'upon': 11, 'beyond': 11, 'bet': 11, 'check': 11, 'doubt': 11, 'admit': 11, 'everyday': 11, 'set': 11, 'face': 11, 'understood': 11, 'becaaause': 11, 'wwwith': 11, 'bbbecause': 11, 'saaaw': 11, 'bbby': 11, 'toooo': 11, 'lllike': 11, 'forgot': 10, 'd': 10, 'cant': 10, 'suffer': 10, 'past': 10, 'show': 10, 've': 10, 'stayed': 10, 'suppose': 10, 'begin': 10, 'xd': 10, 'imagine': 10, 'sit': 10, 'fall': 10, 'fffrom': 10, 'diiid': 10, 'frrrom': 10, 'asss': 10, 'liiike': 10, 'born': 10, 'orrr': 10, 'ttthe': 10, 'prepare': 9, 'change': 9, 'onto': 9, 'depend': 9, 'apologize': 9, 's': 9, 'pay': 9, 'promise': 9, 'Z': 9, 'gather': 9, 'turn': 9, 'down': 9, 'laugh': 9, 'therefore': 9, 'swam': 9, 'ifff': 9, 'theee': 9, 'aaas': 9, 'lack': 8, 'thank': 8, 'throw': 8, 'travel': 8, 'annoy': 8, \"'s\": 8, 'oneself': 8, 'ran': 8, 'cake': 8, 'learnt': 8, 'join': 8, 'happen': 8, 'ok': 8, 'ooor': 8, 'wittth': 8, 'juuust': 8, 'sawww': 8, 'buuut': 8, 'dddid': 8, 'allll': 8, 'neither': 7, 'admire': 7, 'participate': 7, 'earn': 7, 'll': 7, 'afraid': 7, 'sang': 7, 'intend': 7, 'glad': 7, 'disagree': 7, 'didn': 7, 'receive': 7, 'teacher': 7, 'pick': 7, 'unless': 7, 'catch': 7, 'na': 7, 'care': 7, 'follow': 7, 'friend': 7, 'tttook': 7, 'beeeen': 7, 'becauuuse': 7, 'jussst': 7, 'back': 7, 'cameee': 7, 'felttt': 7, \"'t\": 7, 'alsooo': 7, 'jjjust': 7, 'let': 6, 'towards': 6, 'search': 6, 'went': 6, 'chose': 6, 'acknowledge': 6, 'remain': 6, 'th': 6, 'accept': 6, 'ride': 6, 'welcome': 6, 'aim': 6, 'grow': 6, 'introduce': 6, 'japanese': 6, 'exercise': 6, 'celebrate': 6, 'okay': 6, 'meant': 6, 'yen': 6, 'lol': 6, 'hung': 6, 'upload': 6, 'caaame': 6, 'sooo': 6, 'mettt': 6, 'thoughhht': 6, 'nnnow': 6, 'justtt': 6, 'nooot': 6, 'stillll': 6, 'uuup': 6, 'maddde': 6, 'founnnd': 6, 'maaade': 6, 'veryyy': 6, 'quarrel': 5, 'dreamt': 5, 'sym': 5, 'contribute': 5, 'create': 5, 'graduate': 5, 'hit': 5, 'draw': 5, 'wait': 5, 'require': 5, 'handle': 5, 'manage': 5, 'post': 5, 'didnt': 5, 'trust': 5, 'wave': 5, 'serve': 5, 'good': 5, 'adore': 5, 'notice': 5, 'self': 5, 'provide': 5, 'better': 5, 'wherever': 5, 'improve': 5, 'communicate': 5, 'eaten': 5, 'waste': 5, 'cry': 5, 'move': 5, 'occur': 5, 'very': 5, 'worth': 5, 'sumo': 5, 'alssso': 5, 'tookkk': 5, 'oooften': 5, 'advance': 5, 'wwwere': 5, 'home': 5, 'fffelt': 5, 'verrry': 5, 'alllways': 5, 'agooo': 5, 'watcheddd': 5, 'boughttt': 5, 'neveeer': 5, 'likeee': 5, 'founddd': 5, 'eveeer': 5, 'thaaat': 5, 'knewww': 5, 'feeelt': 5, 'mmmet': 5, 'reallyyy': 5, 'l': 4, 'oh': 4, 'built': 4, 'chat': 4, 'marry': 4, 't': 4, 'seek': 4, 'beacuse': 4, 'strange': 4, 'was': 4, 'well': 4, 'compare': 4, 'suggest': 4, 'couldn': 4, 'everybody': 4, 'behave': 4, 'enter': 4, 'mother': 4, 'train': 4, 'hahaha': 4, 'refuse': 4, 'ourself': 4, 're': 4, 'p': 4, 'shouldnt': 4, 'seldom': 4, 'arrive': 4, 'shot': 4, 'return': 4, 'commute': 4, 'assume': 4, 'ls': 4, 'relaxed': 4, 'bite': 4, 'cheer': 4, 'ignore': 4, 'luck': 4, 'brush': 4, 'deal': 4, 'mind': 4, 'rely': 4, \"'cause\": 4, 'haven': 4, 'recognize': 4, 'blame': 4, 'act': 4, 'question': 4, 'cherry': 4, 'feed': 4, 'end': 4, 'represent': 4, 'ume': 4, 'dare': 4, 'sushi': 4, 'ten': 4, 'support': 4, 'curry': 4, 'express': 4, 'fouuund': 4, 'fooound': 4, 'arrround': 4, 'saiiid': 4, 'ssso': 4, 'oftennn': 4, 'busy': 4, 'useeed': 4, 'aaall': 4, 'starteddd': 4, 'vvvery': 4, 'enjoyeddd': 4, 'intooo': 4, 'nottt': 4, 'boughhht': 4, 'ta': 4, 'aaalso': 4, 'madeee': 4, 'playeddd': 4, 'english': 4, 'afterrr': 4, 'absent': 4, 'iiif': 4, 'rain': 4, 'hereee': 4, 'asleep': 4, 'lang': 4, 'afteeer': 4, 'onlyyy': 4, 'arouuund': 4, 'fight': 3, 'emigrate': 3, 'crazy': 3, 'belive': 3, 'smell': 3, 'win': 3, 'offer': 3, 'discover': 3, 'drove': 3, 'haha': 3, 'dress': 3, 'stop': 3, 'treat': 3, 'dream': 3, 'thereupon': 3, 'warm': 3, 'disturb': 3, 'url': 3, 'reach': 3, 'happend': 3, 'o': 3, 'rest': 3, 'j': 3, 'report': 3, 'memorize': 3, 'cough': 3, 'sense': 3, 'affect': 3, 'perfect': 3, 'online': 3, 'deserve': 3, 'driven': 3, 'carry': 3, 'degree': 3, 'stir': 3, 'cherish': 3, 'shop': 3, 'license': 3, 'beause': 3, 'ocean': 3, 'excuse': 3, 'focus': 3, 'eve': 3, 'afterwards': 3, 'touch': 3, 'e': 3, 'translate': 3, 'explain': 3, 'taste': 3, 'korean': 3, 'bike': 3, 'whenever': 3, 'de': 3, 'spread': 3, 'break': 3, 'wow': 3, 'quite': 3, 'chew': 3, 'name': 3, 'answer': 3, 'review': 3, 'didint': 3, 'click': 3, 'transcribe': 3, 'ya': 3, 'cope': 3, 'hard': 3, 'soak': 3, 'brave': 3, 'played': 3, 'criticize': 3, 'spilt': 3, 'soccer': 3, 'color': 3, 'squat': 3, 'mistook': 3, 'fit': 3, 'pm': 3, 'fly': 3, 'thhhis': 3, 'towwward': 3, 'hungry': 3, 'enjoyeeed': 3, 'afffter': 3, 'arriveddd': 3, 'lunch': 3, 'askeeed': 3, 'likkke': 3, 'ppplayed': 3, 'thisss': 3, 'fffound': 3, 'thouggght': 3, 'sick': 3, 'becammme': 3, 'oveeer': 3, 'alive': 3, 'thousand': 3, 'tooold': 3, 'eeever': 3, 'hhhas': 3, 'aaalways': 3, 'basketball': 3, 'stand': 3, 'becccause': 3, 'finisheddd': 3, 'starteeed': 3, 'friends': 3, 'gain': 3, 'ttthis': 3, 'ssstill': 3, 'gaveee': 3, 'bbboth': 3, 'boss': 3, 'wokkke': 3, 'tttoo': 3, 'haaas': 3, 'ttthough': 3, 'dinner': 3, 'aaaround': 3, 'everrry': 3, 'herrre': 3, 'thhhough': 3, 'unnntil': 3, 'forward': 3, 'fun': 3, 'visiteddd': 3, 'known': 3, 'meeet': 3, 'annn': 3, 'thiiis': 3, 'sleepy': 3, 'estimate': 2, 'askesd': 2, 'os': 2, 'athlete': 2, 'okonomiyaki': 2, 'fixed': 2, 'defuse': 2, 'abuse': 2, 'encounter': 2, 'rank': 2, 'climb': 2, 'offen': 2, 'omgsh': 2, 'ha': 2, 'splash': 2, 'confident': 2, 'concentrate': 2, 'crash': 2, 'akind': 2, 'tallk': 2, 'kimono': 2, 'matsutake': 2, 'jump': 2, 'rip': 2, 'art': 2, 'propose': 2, 'fed': 2, 'refer': 2, 'blew': 2, 'beat': 2, 'deeply': 2, 'fourth': 2, 'close': 2, 'zhout': 2, 'pretend': 2, 'but': 2, 'swear': 2, 'apply': 2, 'indicate': 2, 'pronouce': 2, 'colour': 2, 'hade': 2, 'speaks': 2, 'struggle': 2, 'ready': 2, 'fail': 2, 'contrast': 2, 'sometimes': 2, 'cover': 2, 'buckle': 2, 'wiht': 2, 'roll': 2, 'mystery': 2, 'separate': 2, 'were': 2, 'boyfriend': 2, 'x': 2, 'develop': 2, 'rent': 2, 'speakkorean': 2, 'affiliate': 2, 'assemble': 2, 'awesome': 2, 'cross': 2, 'encourage': 2, 'allow': 2, 'couldspeak': 2, 'greet': 2, 'open': 2, 'ah': 2, 'cause': 2, 'studiedenglish': 2, 'nowadays': 2, 'arrange': 2, 'one': 2, 'dear': 2, 'sweat': 2, 'everytime': 2, 'dedicate': 2, 'helps': 2, 'lazy': 2, 'dylan': 2, 'pussy': 2, 'baby': 2, 'burnt': 2, 'smoke': 2, 'canhelp': 2, 'flew': 2, 'accede': 2, 'most': 2, 'adjust': 2, 'lend': 2, 'yoga': 2, 'knowledge': 2, 'thatthat': 2, 'control': 2, 'tomorow': 2, 'exist': 2, 'half': 2, 'sing': 2, 'sore': 2, 'wrap': 2, 'usualy': 2, 'di': 2, 'tore': 2, 'umm': 2, 'someday': 2, 'loves': 2, 'steep': 2, 'harvest': 2, 'clean': 2, 'cave': 2, 'persist': 2, 'land': 2, 'bcause': 2, 'wiil': 2, 'smile': 2, 'misunderstand': 2, 'eventhough': 2, 'motivate': 2, 'preach': 2, 'emit': 2, 'nead': 2, 'mistaken': 2, 'u': 2, 'essay': 2, 'cut': 2, 'possess': 2, 'complain': 2, 'deciede': 2, 'ball': 2, 'realizedyou': 2, 'vary': 2, 'learnd': 2, 'update': 2, 'sharpen': 2, 'ohhh': 2, 'vs': 2, 'whole': 2, 'somehow': 2, 'which': 2, 'sorry': 2, 'distract': 2, 'complexly': 2, 'little': 2, 'bless': 2, 'heavy': 2, 'missjohn': 2, 'rememberd': 2, 'reject': 2, 'anymore': 2, 'looks': 2, 'type': 2, 'bit': 2, 'divide': 2, 'pull': 2, 'regulary': 2, 'record': 2, 'smelt': 2, 'tube': 2, 'havent': 2, 'illuminate': 2, 'fluctuate': 2, 'wild': 2, 'goose': 2, 'peak': 2, 'anytime': 2, 'fry': 2, 'daresay': 2, 'perform': 2, 'yeserday': 2, 'novel': 2, 'kkk': 2, 'bbbefore': 2, 'totallyyy': 2, 'comment': 2, 'free': 2, 'hhheard': 2, 'anxiety': 2, 'sad': 2, 'talkeeed': 2, 'felllt': 2, 'anothhher': 2, 'cold': 2, 'rebuilt': 2, 'aaan': 2, 'shower': 2, 'hasss': 2, 'thumb': 2, 'spent': 2, 'looost': 2, 'relax': 2, 'agggo': 2, 'hearrrd': 2, 'insist': 2, 'nobody': 2, 'tollld': 2, 'offff': 2, 'uuuntil': 2, 'aaafter': 2, 'alllthough': 2, 'hhhere': 2, 'ofteeen': 2, 'deliberate': 2, 'ssseen': 2, 'contest': 2, 'twenty': 2, 'eeexcept': 2, 'eveeen': 2, 'overcome': 2, 'lefttt': 2, 'wrottte': 2, 'whiiile': 2, 'forgotten': 2, 'involve': 2, 'beforrre': 2, 'nooo': 2, 'weeell': 2, 'evvvery': 2, 'ssspent': 2, 'restaurant': 2, 'thrrrough': 2, 'stayeddd': 2, 'taught': 2, 'stole': 2, 'wrooote': 2, 'alwaaays': 2, 'everrr': 2, 'deep': 2, 'wellll': 2, 'lookeddd': 2, 'wwwell': 2, 'stttill': 2, 'whhhile': 2, 'playeeed': 2, 'inspire': 2, 'cammme': 2, 'thoughttt': 2, 'drunk': 2, 'tolddd': 2, 'wanteddd': 2, 'kneeew': 2, 'found': 2, 'enouuugh': 2, 'thhhan': 2, 'frooom': 2, 'ovvver': 2, 'annnother': 2, 'cookeddd': 2, 'chicken': 2, 'wrrrote': 2, 'brown': 2, 'mmmade': 2, 'learning': 2, 'trieddd': 2, 'permit': 2, 'envious': 2, 'saiddd': 2, 'innnto': 2, 'thaaan': 2, 'becaaame': 2, 'overcame': 2, 'incorrect': 2, 'sometimeees': 2, 'theeerefore': 2, 'standardize': 2, 'whillle': 2, 'bornnn': 2, 'alwaysss': 2, 'wrong': 2, 'passeddd': 2, 'aaago': 2, 'watcheeed': 2, 'nice': 2, 'tttold': 2, 'fatal': 2, 'studieeed': 2, 'likeddd': 2, 'untiiil': 2, 'aaany': 2, 'overrr': 2, 'iiinto': 2, 'nnnear': 2, 'sssaid': 2, 'afttter': 2, 'web': 2, 'studieddd': 2, 'leffft': 2, 'remmber': 1, 'kept': 1, 'wannna': 1, 'appriciate': 1, 'wll': 1, 'ayear': 1, 'rub': 1, 'fellbackasleep': 1, 'liqueur': 1, 'experience': 1, 'sung': 1, 'child': 1, 'surround': 1, 'soooooooooooo': 1, 'hang': 1, 'exchange': 1, 'indirect': 1, 'comfirm': 1, 'observe': 1, 'awoke': 1, 'phobic': 1, 'uploadedhis': 1, 'sincerly': 1, 'kinda': 1, 'heehee': 1, 'misunderstood': 1, 'couldnot': 1, 'envelope': 1, 'nbsp': 1, 'did': 1, 'sax': 1, 'fold': 1, 'recite': 1, 'spit': 1, 'host': 1, 'natto': 1, 'shrink': 1, 'washlet': 1, 'tread': 1, 'goodbye': 1, 'later': 1, 'prevent': 1, 'got': 1, 'sho': 1, 'nagging': 1, 'again': 1, 'bore': 1, 'winnie': 1, 'wash': 1, 'dodge': 1, 'obstructing': 1, 'rust': 1, 'convene': 1, 'ct': 1, 'awhile': 1, 'discus': 1, 'fiestas': 1, 'diary': 1, 'appeal': 1, 'tap': 1, 'decid': 1, 'dvd': 1, 'save': 1, 'tall': 1, 'haveto': 1, 'spin': 1, 'bye': 1, 'dine': 1, 'introduction': 1, 'unearth': 1, 'dunno': 1, 'koean': 1, 'teachs': 1, 'shame': 1, 'thin': 1, 'paraphrase': 1, 'aside': 1, 'tomodachi': 1, 'perplex': 1, 'discuss': 1, 'exam': 1, 'xoxo': 1, 'elect': 1, 'settle': 1, 'high': 1, 'cleand': 1, 'blind': 1, 'anyother': 1, 'retreat': 1, 'encourages': 1, 'rock': 1, 'hiphop': 1, 'perspective': 1, 'arrivehome': 1, 'pw': 1, 'store': 1, 'waffle': 1, 'grateful': 1, 'soup': 1, 'fear': 1, 'themto': 1, 'canunderstand': 1, 'kkkkk': 1, 'pmand': 1, 'claim': 1, 'cost': 1, 'wanted': 1, 'met': 1, 'specialize': 1, 'hurry': 1, 'tempura': 1, 'fought': 1, 'sould': 1, 'kidding': 1, 'became': 1, 'harder': 1, 'interior': 1, 'disappear': 1, 'ike': 1, 'mom': 1, 'woman': 1, 'spoke': 1, 'broadcasts': 1, 'selfish': 1, 'stroll': 1, 'working': 1, 'sound': 1, 'athough': 1, 'survive': 1, 'mountain': 1, 'embrace': 1, 'inc': 1, 'dozed': 1, 'narimasu': 1, 'seat': 1, 'aloha': 1, 'stifle': 1, 'grasp': 1, 'bougth': 1, 'befor': 1, 'differentiate': 1, 'alway': 1, 'worte': 1, 'richer': 1, 'allah': 1, 'bow': 1, 'visitedmy': 1, 'attempt': 1, 'hatch': 1, 'teste': 1, 'dyed': 1, 'xo': 1, 'dance': 1, 'osechi': 1, 'influence': 1, 'def': 1, 'willtry': 1, 'havebecome': 1, 'sooooooooooooooooo': 1, 'anyway': 1, 'devote': 1, 'undertstand': 1, 'collect': 1, 'underneath': 1, 'accidentallyfound': 1, 'relevant': 1, 'nowwe': 1, 'evaluate': 1, 'wold': 1, 'display': 1, 'reread': 1, 'paint': 1, 'arise': 1, 'noon': 1, 'gottheiphone': 1, 'akipponn': 1, 'atop': 1, 'coud': 1, 'aber': 1, 'avoid': 1, 'mine': 1, 'shoule': 1, 'sooooooooooooo': 1, 'cu': 1, 'spoil': 1, 'hat': 1, 'request': 1, 'tree': 1, 'recieve': 1, 'iike': 1, 'berry': 1, 'uset': 1, 'surf': 1, 'frist': 1, 'breath': 1, 'respond': 1, 'refrain': 1, 'amazon': 1, 'realy': 1, 'cool': 1, 'count': 1, 'rare': 1, 'applause': 1, 'forgave': 1, 'owe': 1, 'containstolen': 1, 'staffs': 1, 'foreigner': 1, 'father': 1, 'becasue': 1, 'wander': 1, 'puffs': 1, 'hug': 1, 'courtesy': 1, 'occupy': 1, 'congratuate': 1, 'loved': 1, 'alumnus': 1, 'bell': 1, 'com': 1, 'realise': 1, 'figure': 1, 'attitude': 1, 'ribichan': 1, 'writing': 1, 'resolve': 1, 'anxious': 1, 'cast': 1, 'willread': 1, 'corrective': 1, 'weight': 1, 'feltbetter': 1, 'toufuchige': 1, 'match': 1, 'weretoo': 1, 'publish': 1, 'min': 1, 'interviewer': 1, 'nevermind': 1, 'breathe': 1, 'seminar': 1, 'neurosurgery': 1, 'scratch': 1, 'mach': 1, 'best': 1, 'relate': 1, 'pratice': 1, 'yea': 1, 'sincerely': 1, 'volleyball': 1, 'becamelost': 1, 'mede': 1, 'triedto': 1, 'stammer': 1, 'mad': 1, 'leant': 1, 'fiind': 1, 'couldnt': 1, 'agreed': 1, 'san': 1, 'drool': 1, 'relace': 1, 'shooooot': 1, 'ache': 1, 'speech': 1, 'cultivate': 1, 'turnedaround': 1, 'wouldnt': 1, 'log': 1, 'snore': 1, 'skateboad': 1, 'entail': 1, 'againnnst': 1, 'export': 1, 'tt': 1, 'exxxcept': 1, 'samurai': 1, 'chosen': 1, 'shoot': 1, 'threw': 1, 'goint': 1, 'expart': 1, 'somewhat': 1, 'junior': 1, 'stew': 1, 'zombie': 1, 'angry': 1, 'nd': 1, 'eithhher': 1, 'sakura': 1, 'beautiful': 1, 'luuunch': 1, 'emotional': 1, 'short': 1, 'hooowever': 1, 'stomach': 1, 'curious': 1, 'warehouse': 1, 'continueeed': 1, 'cm': 1, 'lit': 1, 'thirsty': 1, 'oily': 1, 'cleaning': 1, 'pod': 1, 'confirmeddd': 1, 'laughhhed': 1, 'thoght': 1, \"'sss\": 1, 'golf': 1, 'flowershave': 1, 'viaaa': 1, 'expensivefor': 1, 'becauseee': 1, 'test': 1, 'ppper': 1, 'nevertheless': 1, 'xbox': 1, 'baught': 1, 'besidesss': 1, 'sake': 1, 'grown': 1, 'loveeed': 1, 'calleddd': 1, 'counteddd': 1, 'turneeed': 1, 'interpret': 1, 'besideees': 1, 'bothhher': 1, 'consist': 1, 'fellll': 1, 'feeell': 1, 'socialise': 1, 'react': 1, 'knowwws': 1, 'nz': 1, 'helddd': 1, 'brings': 1, 'conducteeed': 1, 'verb': 1, 'listeneeed': 1, 'lucky': 1, 'wriiitten': 1, 'discusseeed': 1, 'someeetimes': 1, 'hhheld': 1, 'easy': 1, 'anywayyy': 1, 'isin': 1, 'cinema': 1, 'enougggh': 1, 'fire': 1, 'sinccce': 1, 'soju': 1, 'honer': 1, 'aguess': 1, 'alzheimer': 1, 'fast': 1, 'bbback': 1, 'pink': 1, 'awayyy': 1, 'gggave': 1, 'mistake': 1, 'bend': 1, 'gggot': 1, 'laaaugh': 1, 'restrict': 1, 'eeeach': 1, 'faileeed': 1, 'moveddd': 1, 'escapeeed': 1, 'outtt': 1, 'rude': 1, 'arooound': 1, 'byte': 1, 'spendingyour': 1, 'slip': 1, 'chase': 1, 'ooonto': 1, 'manageddd': 1, 'flown': 1, 'modifieddd': 1, 'sign': 1, 'lateeer': 1, 'agaaainst': 1, 'surviveeed': 1, 'eeevery': 1, 'cute': 1, 'gluten': 1, 'wrrritten': 1, 'proud': 1, 'befffore': 1, 'theeen': 1, 'eager': 1, 'sommme': 1, 'onsen': 1, 'hustle': 1, 'panda': 1, 'trulyyy': 1, 'sometimesss': 1, 'thhherefore': 1, 'sidewalk': 1, 'whilst': 1, 'fix': 1, 'torussia': 1, 'hair': 1, 'anottther': 1, 'han': 1, 'thinkingabout': 1, 'alllready': 1, 'rewrote': 1, 'willdriveyou': 1, 'maybbbe': 1, 'marrieeed': 1, 'everyone': 1, 'callthis': 1, 'postpone': 1, 'debate': 1, 'messy': 1, 'hearddd': 1, 'probabbbly': 1, 'fulfill': 1, 'usuallyyy': 1, 'interlace': 1, 'begaaan': 1, 'openeeed': 1, 'outsiiide': 1, 'comer': 1, 'parent': 1, 'strive': 1, 'volunteer': 1, 'alrrready': 1, 'visiteeed': 1, 'loose': 1, 'podcast': 1, 'althhhough': 1, 'subtitle': 1, 'gothere': 1, 'silent': 1, 'view': 1, 'supposeeed': 1, 'alreaaady': 1, 'w': 1, 'limit': 1, 'almossst': 1, 'ttthere': 1, 'beforeee': 1, 'account': 1, 'hello': 1, 'startto': 1, 'puuut': 1, 'traveleeed': 1, 'sannng': 1, 'overwhelm': 1, 'patch': 1, 'yes': 1, 'workeddd': 1, 'draaank': 1, 'aroma': 1, 'jot': 1, 'lookeeed': 1, 'planteeed': 1, 'orlando': 1, 'alreadddy': 1, 'funny': 1, 'faint': 1, 'mooore': 1, 'respondeddd': 1, 'girlfriend': 1, 'aaare': 1, 'morrre': 1, 'sixty': 1, 'calorie': 1, 'pretty': 1, 'beggeddd': 1, 'twitter': 1, 'forparticular': 1, 'baseeed': 1, 'spppent': 1, 'likes': 1, 'prayed': 1, 'overtime': 1, 'headeeed': 1, 'fourteen': 1, 'repeats': 1, 'nowww': 1, 'laugggh': 1, 'tequila': 1, 'nnnever': 1, 'guideeed': 1, 'boku': 1, 'applieeed': 1, 'first': 1, 'seeeen': 1, 'recoed': 1, 'anooother': 1, 'boreddd': 1, 'firmlyyy': 1, 'extend': 1, 'object': 1, 'sssang': 1, 'anotherrr': 1, 'beaten': 1, 'beeefore': 1, 'bacause': 1, 'operate': 1, 'weather': 1, 'announceeed': 1, 'attthough': 1, 'llll': 1, 'likeeed': 1, 'passeeed': 1, 'stuff': 1, 'yawn': 1, 'yuka': 1, 'geve': 1, 'enough': 1, 'migraine': 1, 'evvver': 1, 'locatid': 1, 'compliment': 1, 'noneee': 1, 'ofrainy': 1, 'quiteee': 1, 'fake': 1, 'anybody': 1, 'neeext': 1, 'hvae': 1, 'ptotect': 1, 'challenge': 1, 'shave': 1, 'barbecue': 1, 'becaussse': 1, 'educate': 1, 'backkk': 1, 'concur': 1, 'download': 1, 'ninja': 1, 'shy': 1, 'hotcake': 1, 'caughhht': 1, 'alreeeady': 1, 'laid': 1, 'rd': 1, 'gotten': 1, 'wordpress': 1, 'warmly': 1, 'siiince': 1, 'stayeeed': 1, 'strain': 1, 'toeic': 1, 'submit': 1, 'imitate': 1, 'steam': 1, 'alwayyys': 1, 'sumou': 1, 'smelleeed': 1, 'caught': 1, 'adzuki': 1, 'etc': 1, 'mixed': 1, 'tv': 1, 'seeent': 1, 'amooong': 1, 'sari': 1, 'begggan': 1, 'hiro': 1, 'tolerant': 1, 'rice': 1, 'attendeeed': 1, 'cccame': 1, 'signal': 1, 'cafe': 1, 'syndrome': 1, 'bought': 1, 'kkknew': 1, 'goth': 1, 'mix': 1, 'bbbeen': 1, 'stiiill': 1, 'trails': 1, 'thought': 1, 'remembereeed': 1, 'nori': 1, 'aaabove': 1, 'heeeard': 1, 'barelyyy': 1, 'alopecia': 1, 'filleeed': 1, 'arriveeed': 1, 'sssometimes': 1, 'bread': 1, 'processeeed': 1, 'enugh': 1, 'againnn': 1, 'lauuugh': 1, 'willfind': 1, 'ppput': 1, 'belowww': 1, 'concave': 1, 'wanteeed': 1, 'drop': 1, 'sooome': 1, 'ofttten': 1, 'reaaad': 1, 'ttthrough': 1, 'bbbroken': 1, 'rivaleddd': 1, 'innocent': 1, 'wassshlet': 1, 'cafes': 1, 'mulberry': 1, 'hellld': 1, 'boooth': 1, 'baaack': 1, 'calm': 1, 'grilleeed': 1, 'attendeddd': 1, 'busier': 1, 'noodle': 1, 'peed': 1, 'speeent': 1, 'inttto': 1, 'payed': 1, 'fur': 1, 'shoud': 1, 'prohibit': 1, 'beeehind': 1, 'coustom': 1, 'present': 1, 'almosttt': 1, 'vampire': 1, 'expecteddd': 1, 'consume': 1, 'jealous': 1, 'hay': 1, 'omlets': 1, 'spennnt': 1, 'forgooot': 1, 'select': 1, 'associate': 1, 'taht': 1, 'comprehend': 1, 'majoreeed': 1, 'ear': 1, 'befooore': 1, 'becouse': 1, 'cannt': 1, 'checkeeed': 1, 'everyyy': 1, 'destroy': 1, 'horror': 1, 'forever': 1, 'thooough': 1, 'driveeed': 1, 'hill': 1, 'sinceee': 1, 'confuse': 1, 'duet': 1, 'extremelyyy': 1, 'sssimply': 1, 'nap': 1, 'cheap': 1, 'images': 1, 'experienced': 1, 'maintain': 1, 'sacrifice': 1, 'nnno': 1, 'heaaard': 1, 'anesthesia': 1, 'ohanami': 1, 'decideddd': 1, 'sensitive': 1, 'pickle': 1, 'raaan': 1, 'spicy': 1, 'betweeeen': 1, 'thoughhh': 1, 'annoyed': 1, 'ronaldinho': 1, 'adopteddd': 1, 'llleft': 1, 'envys': 1, 'leeeft': 1, 'leopard': 1, 'veeery': 1, 'gavvve': 1, 'soooo': 1, 'assess': 1, 'faileddd': 1, 'argue': 1, 'yao': 1, 'fifty': 1, 'hannng': 1, 'restart': 1, 'drowsy': 1, 'recordeeed': 1, 'ooonly': 1, 'gon': 1, 'barcelona': 1, 'moreovvver': 1, 'skype': 1, 'arab': 1, 'eric': 1, 'arounnnd': 1, 'shouldtake': 1, 'coffee': 1, 'rrran': 1, 'add': 1, 'square': 1, 'wor': 1, 'buddhist': 1, 'excepttt': 1, 'talkeddd': 1, 'oppose': 1, 'neaaar': 1, 'yay': 1, 'greeew': 1, 'hid': 1, 'wwwrote': 1, 'eeenough': 1, 'adapteeed': 1, 'smoker': 1, 'cookeeed': 1, 'gaaave': 1, 'screameddd': 1, 'fooorgot': 1, 'hesitate': 1, 'trieeed': 1, 'viiisit': 1, 'thhhroughout': 1, 'bleach': 1, 'spokkke': 1, 'alllmost': 1, 'phoneddd': 1, 'askeddd': 1, 'taughhht': 1, 'aaamong': 1, 'almooost': 1, 'empty': 1, 'entereeed': 1, 'finnish': 1, 'soooon': 1, 'fine': 1, 'seven': 1, 'twritten': 1, 'havee': 1, 'arrang': 1, 'excitement': 1, 'frozen': 1, 'mend': 1, 'treid': 1, 'nnna': 1, 'plenty': 1, 'haiku': 1, 'spinach': 1, 'likedthem': 1, 'adult': 1, 'atyang': 1, 'acupuncture': 1, 'althougggh': 1, 'purpose': 1, 'cleaneeed': 1, 'seldooom': 1, 'frommm': 1, 'matches': 1, 'muscular': 1, 'tought': 1, 'tooth': 1, 'conduct': 1, 'soundsss': 1, 'writttten': 1, 'practiceddd': 1, 'ill': 1, 'washeeed': 1, 'photos': 1, 'lossst': 1, 'foresee': 1, 'kepppt': 1, 'timid': 1, 'song': 1, 'jhon': 1, 'useddd': 1, 'sssome': 1, 'agaaain': 1, 'moveeed': 1, 'ooout': 1, 'cjs': 1, 'definitelyyy': 1, 'n': 1, 'solve': 1, 'lesson': 1, 'duringgg': 1, 'enchiladas': 1, 'jugon': 1, 'achieve': 1, 'thhhrough': 1, 'drawn': 1, 'paiddd': 1, 'bottth': 1, 'chatteddd': 1, 'nodame': 1, 'firssst': 1, 'ooover': 1, 'roameeed': 1, 'decideeed': 1, 'produce': 1, 'hunt': 1, 'transmit': 1, 'language': 1, 'abooove': 1, 'registereddd': 1, 'massages': 1, 'delicious': 1, 'yourjapanese': 1, 'dormitory': 1, 'beennn': 1, 'betweennn': 1, 'panegyrized': 1, 'igo': 1, 'paaaid': 1, 'donnne': 1, 'playsss': 1, 'ouuutside': 1, 'alone': 1, 'cynical': 1, 'genius': 1, 'design': 1, 'twiceee': 1, 'closest': 1, 'float': 1, 'becase': 1, 'underssstood': 1, 'cup': 1, 'video': 1, 'impatient': 1, 'club': 1, 'lead': 1, 'freshman': 1, 'booo': 1, 'oooff': 1, 'soo': 1, 'irori': 1, 'nooow': 1, 'beeeacuse': 1, 'causeddd': 1, 'risk': 1, 'cognition': 1, 'randomly': 1, 'cuuut': 1, 'desire': 1, 'withhhout': 1, 'alwwways': 1, 'notslept': 1, 'doooes': 1, 'hearthe': 1, 'bear': 1, 'wherein': 1, 'mmmuch': 1, 'release': 1, 'stimulate': 1, 'till': 1, 'organize': 1, 'gaineeed': 1, 'katz': 1, 'chatteeed': 1, 'sssaw': 1, 'neeearby': 1, 'thereee': 1, 'seafood': 1, 'expensive': 1, 'concert': 1, 'blue': 1, 'drinks': 1, 'increase': 1, 'spokeee': 1, 'thailand': 1, 'gooone': 1, 'ankle': 1, 'besideee': 1, 'osore': 1, 'overweigh': 1, 'happy': 1, 'alllumnus': 1, 'nowadaaays': 1, 'spray': 1, 'dddrew': 1, 'aaagain': 1})\n"
     ]
    }
   ],
   "source": [
    "words_counter = collections.Counter([word  for i in range(len(trainSet)) for word in trainSet[i][0].split()])\n",
    "#print(words_counter)\n",
    "print('{} English words.'.format(len([word  for i in range(len(trainSet)) for word in trainSet[i][0].split()])))\n",
    "print('{} unique English words.'.format(len(words_counter)))\n",
    "print(words_counter.most_common(10))\n",
    "print(words_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dt': 2, 'the': 3, 'nn': 4, 'pos': 5, 'vbz': 6, '``': 7, 'nnp': 8, 'prp': 9, 'it': 10, 'vbd': 11, 'rb': 12, 'jj': 13, 'this': 14, 'they': 15, 'vbp': 16, 'are': 17, 'innn': 18, 'for': 19, 'me': 20, 'wrb': 21, 'to': 22, 'vb': 23, ',': 24, 'cc': 25, 'nns': 26, 'in': 27, 'ex': 28, 'a': 29, 'P': 30, 'we': 31, 'think': 32, 'prpd': 33, 'our': 34, 'my': 35, 'i': 36, 'am': 37, 'along': 38, 'W': 39, 'md': 40, 'V': 41, 'that': 42, 'have': 43, 'every': 44, 'go': 45, 'want': 46, 'an': 47, 'A': 48, 'cd': 49, 'some': 50, 'you': 51, 'she': 52, 'so': 53, 'all': 54, 'about': 55, 'vbg': 56, 'from': 57, 'D': 58, 'of': 59, 'them': 60, 'on': 61, 'with': 62, 'vbn': 63, 'K': 64, 'hope': 65, 'S': 66, 'his': 67, 'N': 68, 'like': 69, 'T': 70, 'heard': 71, 'feel': 72, 'if': 73, 'as': 74, 'belong': 75, 'because': 76, 'before': 77, 'jjs': 78, 'keep': 79, 'read': 80, 'L': 81, 'their': 82, 'he': 83, 'since': 84, 'wp': 85, 'around': 86, 'your': 87, 'her': 88, 'expect': 89, 'at': 90, 'need': 91, 'M': 92, 'wdt': 93, 'outside': 94, 'B': 95, 'do': 96, 'jjr': 97, 'rp': 98, 'between': 99, 'no': 100, 'these': 101, 'love': 102, 'use': 103, 'J': 104, 'F': 105, 'see': 106, 'b': 107, 'after': 108, 'suppose': 109, 'ate': 110, 'C': 111, 'behind': 112, 'I': 113, 'Y': 114, 'rbs': 115, 'H': 116, 'than': 117, 'G': 118, 'both': 119, 'pdt': 120, 'listen': 121, 'eat': 122, 'ago': 123, 'above': 124, 'try': 125, 'tell': 126, 'another': 127, 'myself': 128, 'get': 129, 'find': 130, 'speak': 131, 'by': 132, 'stayed': 133, 'look': 134, 'him': 135, 'near': 136, 'drank': 137, 'know': 138, 'seem': 139, 'rbr': 140, 'd': 141, 'write': 142, 'visit': 143, 'become': 144, 'any': 145, 'regret': 146, 'over': 147, 'though': 148, 'without': 149, 'whether': 150, 'into': 151, 'against': 152, 'study': 153, 'take': 154, 'wish': 155, 'confuse': 156, 'while': 157, 'among': 158, 'uh': 159, 'live': 160, 'nnps': 161, 'awesome': 162, 'learn': 163, 'R': 164, 'strange': 165, 'make': 166, 'besides': 167, 'enjoy': 168, 'during': 169, 'haven': 170, 'desire': 171, 'until': 172, 'each': 173, 'care': 174, 'us': 175, 'worry': 176, 'start': 177, 'quit': 178, 'move': 179, 'guys': 180, 'watch': 181, 'himself': 182, 'forgot': 183, 'put': 184, 'say': 185, 'roll': 186, 'brush': 187, 'up': 188, 'down': 189, 'talk': 190, 'onto': 191, 'its': 192, 'wan': 193, 'lose': 194, 'welcome': 195, 'laugh': 196, 'give': 197, 'understand': 198, 'set': 199, 'believe': 200, 'appreciate': 201, 'wrong': 202, 'admit': 203, 'miss': 204, 'bit': 205, 'help': 206, 'buy': 207, 'don': 208, 'hear': 209, 'those': 210, 'next': 211, 'come': 212, 'sing': 213, 'cherry': 214, 'remain': 215, 'E': 216, 'hit': 217, 'choose': 218, 'mean': 219, 'work': 220, 'play': 221, 'guess': 222, 'wear': 223, 'yourself': 224, 'through': 225, 'recommend': 226, 'thank': 227, 'earn': 228, 'chat': 229, 'pay': 230, 'within': 231, 'O': 232, 'stay': 233, 'finish': 234, 'hurt': 235, 'm': 236, 'drink': 237, 'graduate': 238, 'occur': 239, 'nowadays': 240, 'exercise': 241, 'mixed': 242, 'pass': 243, 'lend': 244, 'under': 245, 'begin': 246, 'unless': 247, 'throughout': 248, 'although': 249, 'run': 250, 'quite': 251, 'suggest': 252, 'leave': 253, 'drive': 254, 'wait': 255, 'beside': 256, 'ask': 257, 'father': 258, 'face': 259, 'change': 260, 'themselves': 261, 'wonder': 262, 'appear': 263, 'hate': 264, 'remember': 265, 'respect': 266, 'please': 267, 'out': 268, 'except': 269, 'gather': 270, 'prefer': 271, 'show': 272, 'off': 273, 'draw': 274, 'overslept': 275, 'offer': 276, 'spend': 277, 'cant': 278, 'introduce': 279, 'spread': 280, 'rest': 281, 'enjoyed': 282, 'meet': 283, 'unlike': 284, 'slept': 285, 'educate': 286, 'contribute': 287, 'better': 288, 'across': 289, 'digit': 290, 'display': 291, 'ourselves': 292, 'correct': 293, 'towards': 294, 'taste': 295, 'didn': 296, 'understood': 297, 'agree': 298, 'meant': 299, 'encourage': 300, 'rode': 301, 'eaten': 302, 'friend': 303, 'taught': 304, 'feed': 305, 'travel': 306, 'aim': 307, 'happen': 308, 'envy': 309, 'fw': 310, 'teach': 311, 'hold': 312, 'forget': 313, 'send': 314, 'mother': 315, 'degree': 316, 'ume': 317, 'chose': 318, 'answer': 319, 'upon': 320, 'plan': 321, 'admire': 322, 'wore': 323, 'call': 324, 'wake': 325, 'cherish': 326, 'explain': 327, 'walk': 328, 'dress': 329, 'doubt': 330, 'fall': 331, 'enter': 332, \"''\": 333, 'sushi': 334, 'realize': 335, 'depend': 336, 'recognize': 337, 'dont': 338, \"'s\": 339, 'felt': 340, 'below': 341, 'overcome': 342, 'soak': 343, 'join': 344, 'return': 345, 'recieve': 346, 'shoud': 347, 'played': 348, 'deep': 349, 'continue': 350, 'therefore': 351, 'inside': 352, 'someday': 353, 'cook': 354, 'broth': 355, 've': 356, 'assume': 357, 'let': 358, 'via': 359, 'lack': 360, 'spent': 361, 'per': 362, 'excuse': 363, 'matsutake': 364, 'sit': 365, 'sumo': 366, 's': 367, '$': 368, 'serve': 369, 'rent': 370, 'itself': 371, 'follow': 372, 'learnt': 373, 'wherever': 374, 'create': 375, 'yen': 376, 'bring': 377, 'fun': 378, 'tend': 379, 'adore': 380, 'complain': 381, 'xd': 382, 'promise': 383, 'll': 384, 'despite': 385, 'catch': 386, 'cry': 387, 'imagine': 388, 'decide': 389, 'past': 390, 'upload': 391, 'require': 392, 'sym': 393, 'intend': 394, 'sleep': 395, 'neither': 396, 'hung': 397, 'turn': 398, 'bet': 399, 'ignore': 400, 'manage': 401, 'p': 402, 'bite': 403, 'l': 404, 'insist': 405, 'warm': 406, 'ourself': 407, 'everyday': 408, 'grow': 409, 'back': 410, 'built': 411, 'toward': 412, 'stop': 413, 'die': 414, 'afraid': 415, 'curry': 416, 'license': 417, 'commute': 418, 'sick': 419, 'thousand': 420, 'ran': 421, 'divide': 422, 'english': 423, 'pray': 424, 'swear': 425, 'beat': 426, 'kimono': 427, 'owe': 428, 'seldom': 429, 'well': 430, 'w': 431, 'memorize': 432, 'report': 433, \"'cause\": 434, 'pick': 435, 'provide': 436, 'act': 437, 'check': 438, 'sell': 439, 'dislike': 440, 'lazy': 441, 'limit': 442, 'online': 443, 'ya': 444, 'stand': 445, 'canhelp': 446, 'half': 447, 'end': 448, 'communicate': 449, 'ls': 450, 'notice': 451, 'either': 452, 'de': 453, 'chew': 454, 'celebrate': 455, 'behave': 456, 'pink': 457, 'ride': 458, 'relaxed': 459, 'translate': 460, 'compare': 461, 'didnt': 462, 'busy': 463, 'lol': 464, 'close': 465, 'o': 466, 'sound': 467, 'receive': 468, 'cake': 469, 'quarrel': 470, 'perfect': 471, 'fit': 472, 'sang': 473, 'seek': 474, 'herself': 475, 'beyond': 476, 'color': 477, 'annoy': 478, 're': 479, 'hard': 480, 'ok': 481, 'control': 482, 'consider': 483, 'bread': 484, 'dream': 485, 'wow': 486, 'lead': 487, 'deal': 488, 'handle': 489, 'luck': 490, 'anytime': 491, 'refuse': 492, 'rely': 493, 'brave': 494, 'dare': 495, 'hahaha': 496, 'add': 497, 't': 498, 'snow': 499, 'prepare': 500, 'dreamt': 501, 'attend': 502, 'teacher': 503, 'disagree': 504, 'save': 505, 'suffer': 506, 'post': 507, 'fly': 508, 'self': 509, 'deeply': 510, 'oh': 511, 'inspire': 512, 'apologize': 513, 'criticize': 514, 'japanese': 515, 'relax': 516, 'proud': 517, 'improve': 518, 'drunk': 519, 'waste': 520, 'crazy': 521, 'name': 522, 'perform': 523, 'sometimes': 524, 'trust': 525, 'th': 526, 'accept': 527, 'oneself': 528, 'till': 529, 'participate': 530, 'collect': 531, 'amid': 532, 'looks': 533, 'smell': 534, 'search': 535, 'yoga': 536, 'train': 537, 'marry': 538, 'Z': 539, 'swam': 540, 'carry': 541, 'u': 542, 'type': 543, 'gon': 544, 'essay': 545, 'cave': 546, 'glad': 547, 'sooo': 548, 'wereee': 549, 'playeddd': 550, 'sawww': 551, 'withhh': 552, 'beeecause': 553, 'thhhat': 554, 'wasss': 555, 'innnto': 556, 'iiis': 557, 'ttthat': 558, 'ooof': 559, 'eveeer': 560, 'offf': 561, 'fffelt': 562, 'waaas': 563, 'wanteeed': 564, 'basketball': 565, 'abouttt': 566, 'abooout': 567, 'byyy': 568, 'home': 569, 'iiif': 570, 'fffor': 571, 'iiinto': 572, 'wwwent': 573, 'weeent': 574, 'aaa': 575, 'aaaround': 576, 'boughttt': 577, 'weeere': 578, 'becauuuse': 579, 'bbbut': 580, 'becaaause': 581, 'maddde': 582, 'aaand': 583, 'isss': 584, 'aaas': 585, 'aaat': 586, 'onlyyy': 587, 'allll': 588, 'feeelt': 589, 'na': 590, 'aaalways': 591, 'bbbecause': 592, 'korean': 593, 'jjjust': 594, 'dddid': 595, 'aaalso': 596, 'beeeen': 597, 'haaad': 598, 'anddd': 599, 'n': 600, 'abbbout': 601, 'annnd': 602, 'leffft': 603, 'good': 604, 'saaaw': 605, 'wenttt': 606, 'wennnt': 607, 'aaabout': 608, 'alwaysss': 609, 'enjoyeeed': 610, 'onnn': 611, 'attt': 612, 'toooo': 613, 'watcheeed': 614, 'juuust': 615, 'abouuut': 616, 'fouuund': 617, 'buttt': 618, 'fooor': 619, 'founddd': 620, 'ssso': 621, 'forrr': 622, 'wokkke': 623, 'ooon': 624, 'liiike': 625, 'ttthe': 626, 'haddd': 627, 'veryyy': 628, 'alssso': 629, 'ttthis': 630, 'orrr': 631, 'whillle': 632, 'buuut': 633, 'alwayyys': 634, 'lllike': 635, 'becccause': 636, 'cccame': 637, 'frrrom': 638, 'diddd': 639, 'verrry': 640, 'diiid': 641, 'wwwas': 642, 'trieddd': 643, 'thattt': 644, 'becammme': 645, 'stillll': 646, 'saiddd': 647, 'starteddd': 648, 'begggan': 649, 'absent': 650, 'gottt': 651, 'oooften': 652, 'gooot': 653, 'arouuund': 654, 'eeevery': 655, 'bbby': 656, 'wiiith': 657, 'afterrr': 658, 'chicken': 659, 'fffrom': 660, 'mmmet': 661, 'justtt': 662, 'sleepy': 663, 'iiin': 664, 'found': 665, 'tt': 666, 'reallyyy': 667, 'ten': 668, 'trieeed': 669, 'jussst': 670, 'sssometimes': 671, 'tttoo': 672, 'wwwere': 673, 'thoughhht': 674, 'ssstill': 675, 'tooook': 676, 'nnnow': 677, 'oftennn': 678, 'hungry': 679, 'cammme': 680, 'wittth': 681, 'knewww': 682, 'arrround': 683, 'tookkk': 684, 'wwwith': 685, 'boughhht': 686, 'alone': 687, 'werrre': 688, 'wave': 689, 'shop': 690, 'alwaaays': 691, 'vvvery': 692, 'everrry': 693, 'thiiis': 694, 'saiiid': 695, 'agggo': 696, 'starteeed': 697, 'aaan': 698, 'likkke': 699, 'fooound': 700, 'nnnear': 701, 'weeell': 702, 'asleep': 703, 'whhhile': 704, 'ooor': 705, 'thhhis': 706, 'useddd': 707, 'cold': 708, 'thhhe': 709, 'asss': 710, 'soccer': 711, 'overrr': 712, 'becaussse': 713, 'deserve': 714, 'gaaave': 715, 'friends': 716, 'thaaan': 717, 'almossst': 718, 'ta': 719, 'veeery': 720, 'ifff': 721, 'alllso': 722, 'frooom': 723, 'watcheddd': 724, 'caaame': 725, 'thaaat': 726, 'enjoyeddd': 727, 'useeed': 728, 'arriveddd': 729, 'whiiile': 730, 'onccce': 731, 'likeee': 732, 'dinner': 733, 'studieeed': 734, 'maaade': 735, 'aaall': 736, 'frommm': 737, 'agooo': 738, 'thouggght': 739, 'becauseee': 740, 'madeee': 741, 'fellll': 742, 'cameee': 743, 'stayeddd': 744, 'tttook': 745, 'hhhas': 746, 'afffter': 747, 'expensive': 748, 'felllt': 749, 'mmmade': 750, 'befffore': 751, 'aaago': 752, 'aaany': 753, 'thrrrough': 754, 'askeeed': 755, 'brown': 756, 'theee': 757, 'lang': 758, 'uuup': 759, 'wwwell': 760, 'felttt': 761, 'founnnd': 762, 'support': 763, 'mettt': 764, 'thisss': 765, 'wanteddd': 766, 'visiteddd': 767, 'alllways': 768, 'tooold': 769, 'playeeed': 770, 'alsooo': 771, 'beennn': 772, 'born': 773, 'arooound': 774, 'gggot': 775, 'neveeer': 776, \"'sss\": 777, 'hearrrd': 778, 'alreeeady': 779, 'pain': 780, 'sooome': 781, 'meeet': 782, 'sorry': 783, 'little': 784, 'passeeed': 785, 'studieddd': 786, \"'t\": 787, 'finisheddd': 788, 'nooot': 789, 'ofteeen': 790, 'ttthough': 791, 'thoughttt': 792, 'fffound': 793, 'thhhough': 794, 'lunch': 795, 'advance': 796, 'unnntil': 797, 'nnnot': 798, 'hereee': 799, 'ppput': 800, 'afttter': 801, 'worth': 802, 'sometimeees': 803, 'which': 804, 'alive': 805, 'lookeeed': 806, 'forgotten': 807, 'wrrrote': 808, 'nottt': 809, 'gggave': 810, 'agaiiin': 811, 'intooo': 812, 'solve': 813, 'alwwways': 814, 'aaanother': 815, 'puttt': 816, 'wwwrote': 817, 'unk': 1}\n"
     ]
    }
   ],
   "source": [
    "def tokenize(vocab,sentences):\n",
    "    temp = {}\n",
    "    for w,freq in (vocab.items()):\n",
    "        if(freq > 2):\n",
    "            temp[w] = freq\n",
    "        \n",
    "                    \n",
    "    vocab = temp\n",
    "    vocab_to_int = {word: ii for ii, word in enumerate(vocab, 2)} #reserving one for unknown\n",
    "    vocab_to_int['unk'] = 1\n",
    "    dataset_int = []\n",
    "    for sent in sentences:\n",
    "        #print(sent)\n",
    "        sentSeq = []\n",
    "        for word in sent.split():\n",
    "            if word in vocab_to_int:\n",
    "                sentSeq.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentSeq.append(vocab_to_int['unk'])\n",
    "        #print(sentSeq)\n",
    "        dataset_int.append(sentSeq)\n",
    "    return dataset_int,vocab_to_int\n",
    "\n",
    "text_tokenized, text_tokenizer = tokenize(words_counter,trainSet[:,0])\n",
    "print(text_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef tokenize(vocab,sentences):\\n    vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\\n    #print(vocab)\\n    dataset_int = []\\n    for sent in sentences:\\n        dataset_int.append([vocab_to_int[word] for word in sent.split()])\\n    return dataset_int,vocab_to_int\\n\\ntext_tokenized, text_tokenizer = tokenize(words_counter,trainSet[:,0])\\nprint(text_tokenizer)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def tokenize(vocab,sentences):\n",
    "    vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "    #print(vocab)\n",
    "    dataset_int = []\n",
    "    for sent in sentences:\n",
    "        dataset_int.append([vocab_to_int[word] for word in sent.split()])\n",
    "    return dataset_int,vocab_to_int\n",
    "\n",
    "text_tokenized, text_tokenizer = tokenize(words_counter,trainSet[:,0])\n",
    "print(text_tokenizer)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "[ 2 14  4  6 13 26 24 26 24 25 26  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "def pad(x, length=None):\n",
    "       \n",
    "    if length is None:\n",
    "        \n",
    "        length = max([len(sentence) for sentence in x])\n",
    "        print(length)\n",
    "        \n",
    "    return length,pad_sequences(x, maxlen=length, padding='post')\n",
    "# Pad Tokenized output\n",
    "\n",
    "seqLen,pad_features = pad(text_tokenized)\n",
    "print(pad_features[5])\n",
    "print(seqLen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "818\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(text_tokenizer)+1 # adding 1 for zero value added in padding\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#print(pad_features)\n",
    "#seqLen = 34\n",
    "def hotcodSeq(pad_features):\n",
    "    \n",
    "    temp2 = np.zeros((len(pad_features),seqLen,vocab_size))\n",
    "\n",
    "    for i_seq in range((len(pad_features))):\n",
    "        #print(\"ii \"+str(i_seq))\n",
    "        #print(pad_features[i_seq])\n",
    "        for j_id in range(len(pad_features[i_seq])):\n",
    "            if(pad_features[i_seq][j_id]==18):\n",
    "                temp2[i_seq][j_id][pad_features[i_seq][j_id]] = 1\n",
    "            else:\n",
    "                temp2[i_seq][j_id][pad_features[i_seq][j_id]] = 1\n",
    "        \n",
    "        \n",
    "    return temp2\n",
    "    \n",
    "\n",
    "features_x = hotcodSeq(pad_features)\n",
    "print(features_x[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(96000,)\n"
     ]
    }
   ],
   "source": [
    "labels = np.array(trainSet[:,1])\n",
    "labels = labels.astype(int)\n",
    "print(labels[0])\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(14567)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (4000, 24, 50)            173800    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (4000, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (4000, 64)                3264      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (4000, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (4000, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (4000, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 199,626\n",
      "Trainable params: 199,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense,Dropout,InputLayer,Embedding\n",
    "from keras.layers.core import Flatten\n",
    "def lstmModel(batchSize):\n",
    "    \n",
    "    model = Sequential()\n",
    "    batch_size = batchSize\n",
    "    #model.add(Embedding(80, 10, input_length=seqLen))\n",
    "    #model.add(LSTM(20,return_sequences=True,dropout=0.1,recurrent_dropout=0.1))#output dimensions batch_size,timesteps,20\n",
    "    model.add(LSTM(50,batch_input_shape=(batch_size,seqLen,vocab_size),return_sequences=True,dropout=0.1,recurrent_dropout=0.1))#output dimensions batch_size,timesteps,20\n",
    "    model.add(LSTM(50))\n",
    "    #model.add(Flatten())\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(32,activation='relu'))\n",
    "    model.add(Dense(8,activation='relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    #model.add(Dense(1,activation='tanh'))\n",
    "    #model.add(Dense(1,activation='sigmoid'))\n",
    "    #model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "    return model\n",
    "model = lstmModel(4000)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "52000/96000 [===============>..............] - ETA: 1:48 - loss: 0.6891 - acc: 0.5336"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(features_x,labels,epochs=500,batch_size=4000,shuffle=True,verbose=1)\n",
    "#embedFeatures = np.reshape(pad_features,(len(pad_features),seqLen))\n",
    "#model.fit(embedFeatures,labels,epochs=50,batch_size=10,shuffle=True,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "100000/100000 [==============================] - 3s 32us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4403184786438942, 0.7725400000810623]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.metrics_names)\n",
    "\n",
    "model.evaluate(features_x,labels,batch_size=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000,)\n"
     ]
    }
   ],
   "source": [
    "def text_2_id(tokenizer,sentences):\n",
    "    dataset_int = []\n",
    "    for sent in sentences:\n",
    "        #print(sent)\n",
    "        sentSeq = []\n",
    "        for word in sent.split():\n",
    "            if word in tokenizer:\n",
    "                sentSeq.append(tokenizer[word])\n",
    "            else:\n",
    "                sentSeq.append(tokenizer['unk'])\n",
    "        #print(sentSeq)\n",
    "        dataset_int.append(sentSeq)\n",
    "            \n",
    "    #for sent in sentences:\n",
    "        #dataset_int.append([vocab_to_int[w] for word in sent.split() if (word in vocab_to_int):w = word ])\n",
    "    return dataset_int\n",
    "\n",
    "testSeq = text_2_id(text_tokenizer,testSet[:,0])\n",
    "seqLen,testPadSeq = pad(testSeq,seqLen)\n",
    "#print(testPadSeq)\n",
    "testLabels = np.array(testSet[:,1])\n",
    "#print(testPadSeq)\n",
    "test_features = hotcodSeq(testPadSeq)\n",
    "print(testLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "30000/30000 [==============================] - 1s 30us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5203609764575958, 0.746833344300588]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.metrics_names)\n",
    "#model.evaluate(test_features[0:200,],testLabels[0:200,],batch_size=100)\n",
    "model.evaluate(test_features,testLabels,batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model = lstmModel(1)\n",
    "# copy weights\n",
    "old_weights = model.get_weights()\n",
    "#print(old_weights)\n",
    "pred_model.set_weights=(old_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49446923 0.5055308 ]]\n",
      "0\n",
      "[[0.49446136 0.50553864]]\n",
      "0\n",
      "[[0.4944567 0.5055433]]\n",
      "0\n",
      "[[0.49444196 0.505558  ]]\n",
      "0\n",
      "[[0.4944567 0.5055433]]\n",
      "0\n",
      "[[0.49444196 0.505558  ]]\n",
      "0\n",
      "[[0.4944567 0.5055433]]\n",
      "0\n",
      "[[0.49435824 0.5056418 ]]\n",
      "0\n",
      "[[0.49446923 0.5055308 ]]\n",
      "0\n",
      "[[0.49447274 0.5055272 ]]\n",
      "0\n",
      "[[0.49444196 0.505558  ]]\n",
      "0\n",
      "[[0.49444214 0.50555784]]\n",
      "0\n",
      "[[0.49442554 0.50557446]]\n",
      "0\n",
      "[[0.49446082 0.5055391 ]]\n",
      "0\n",
      "[[0.49429163 0.50570834]]\n",
      "0\n",
      "[[0.49435434 0.50564575]]\n",
      "0\n",
      "[[0.49441588 0.5055842 ]]\n",
      "0\n",
      "[[0.49441588 0.5055842 ]]\n",
      "0\n",
      "[[0.49435893 0.50564104]]\n",
      "0\n",
      "[[0.4944567 0.5055433]]\n",
      "0\n",
      "[[0.49443227 0.5055677 ]]\n",
      "0\n",
      "[[0.49443996 0.50556   ]]\n",
      "0\n",
      "[[0.49442384 0.5055762 ]]\n",
      "0\n",
      "[[0.4944567 0.5055433]]\n",
      "0\n",
      "[[0.49436483 0.50563514]]\n",
      "0\n",
      "[[0.49433142 0.5056686 ]]\n",
      "0\n",
      "[[0.4944567 0.5055433]]\n",
      "0\n",
      "[[0.49444884 0.5055512 ]]\n",
      "0\n",
      "[[0.4943908 0.5056092]]\n",
      "0\n",
      "[[0.49444216 0.5055579 ]]\n",
      "0\n",
      "[[0.49445826 0.50554174]]\n",
      "0\n",
      "[[0.49439937 0.50560063]]\n",
      "0\n",
      "[[0.4945357 0.5054643]]\n",
      "0\n",
      "[[0.49445447 0.5055456 ]]\n",
      "0\n",
      "[[0.49445447 0.5055456 ]]\n",
      "0\n",
      "[[0.4944442  0.50555575]]\n",
      "0\n",
      "[[0.49449354 0.5055064 ]]\n",
      "0\n",
      "[[0.50063634 0.49936366]]\n",
      "0\n",
      "[[0.49538246 0.5046175 ]]\n",
      "0\n",
      "[[0.49456877 0.50543123]]\n",
      "0\n",
      "[[0.49442554 0.50557446]]\n",
      "0\n",
      "[[0.49445468 0.5055454 ]]\n",
      "0\n",
      "[[0.49446923 0.5055308 ]]\n",
      "0\n",
      "[[0.49446923 0.5055308 ]]\n",
      "0\n",
      "[[0.49446923 0.5055308 ]]\n",
      "0\n",
      "[[0.49446923 0.5055308 ]]\n",
      "0\n",
      "[[0.4944567 0.5055433]]\n",
      "0\n",
      "[[0.49444196 0.505558  ]]\n",
      "0\n",
      "[[0.49444196 0.505558  ]]\n",
      "0\n",
      "[[0.49446923 0.5055308 ]]\n",
      "0\n",
      "[[0.49446923 0.5055308 ]]\n",
      "1\n",
      "[[0.49446136 0.50553864]]\n",
      "1\n",
      "[[0.4944567 0.5055433]]\n",
      "1\n",
      "[[0.49444196 0.505558  ]]\n",
      "1\n",
      "[[0.4944567 0.5055433]]\n",
      "1\n",
      "[[0.49444196 0.505558  ]]\n",
      "1\n",
      "[[0.4944567 0.5055433]]\n",
      "1\n",
      "[[0.49435824 0.5056418 ]]\n",
      "1\n",
      "[[0.49446923 0.5055308 ]]\n",
      "1\n",
      "[[0.49447274 0.5055272 ]]\n",
      "1\n",
      "[[0.49444196 0.505558  ]]\n",
      "1\n",
      "[[0.49444214 0.50555784]]\n",
      "1\n",
      "[[0.49442554 0.50557446]]\n",
      "1\n",
      "[[0.49446082 0.5055391 ]]\n",
      "1\n",
      "[[0.49429163 0.50570834]]\n",
      "1\n",
      "[[0.49435434 0.50564575]]\n",
      "1\n",
      "[[0.49441588 0.5055842 ]]\n",
      "1\n",
      "[[0.49435893 0.50564104]]\n",
      "1\n",
      "[[0.4944567 0.5055433]]\n",
      "1\n",
      "[[0.49443227 0.5055677 ]]\n",
      "1\n",
      "[[0.49443996 0.50556   ]]\n",
      "1\n",
      "[[0.49442384 0.5055762 ]]\n",
      "1\n",
      "[[0.4944567 0.5055433]]\n",
      "1\n",
      "[[0.49436483 0.50563514]]\n",
      "1\n",
      "[[0.49433142 0.5056686 ]]\n",
      "1\n",
      "[[0.4944567 0.5055433]]\n",
      "1\n",
      "[[0.49444884 0.5055512 ]]\n",
      "1\n",
      "[[0.4943908 0.5056092]]\n",
      "1\n",
      "[[0.49444216 0.5055579 ]]\n",
      "1\n",
      "[[0.49445826 0.50554174]]\n",
      "1\n",
      "[[0.49439937 0.50560063]]\n",
      "1\n",
      "[[0.4945357 0.5054643]]\n",
      "1\n",
      "[[0.49445447 0.5055456 ]]\n",
      "1\n",
      "[[0.49445447 0.5055456 ]]\n",
      "1\n",
      "[[0.4944442  0.50555575]]\n",
      "1\n",
      "[[0.49449354 0.5055064 ]]\n",
      "1\n",
      "[[0.50063634 0.49936366]]\n",
      "1\n",
      "[[0.49538246 0.5046175 ]]\n",
      "1\n",
      "[[0.4946787  0.50532126]]\n",
      "1\n",
      "[[0.49456877 0.50543123]]\n",
      "1\n",
      "[[0.49442554 0.50557446]]\n",
      "1\n",
      "[[0.49445468 0.5055454 ]]\n",
      "1\n",
      "[[0.49446923 0.5055308 ]]\n",
      "1\n",
      "[[0.49446923 0.5055308 ]]\n",
      "1\n",
      "[[0.49446923 0.5055308 ]]\n",
      "1\n",
      "[[0.49446923 0.5055308 ]]\n",
      "1\n",
      "[[0.4944567 0.5055433]]\n",
      "1\n",
      "[[0.49444196 0.505558  ]]\n",
      "1\n",
      "[[0.49444196 0.505558  ]]\n",
      "1\n",
      "[[0.49446923 0.5055308 ]]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "realLab = []\n",
    "predLab = []\n",
    "for i in range(len(trainSet)):\n",
    "    testX, testy = features_x[i], trainSet[i][1]\n",
    "    testX = testX.reshape(1, seqLen, vocab_size)\n",
    "    #predy = pred_model.predict(testX,batch_size=1).argmax(axis=-1)[0]\n",
    "    predy = pred_model.predict(testX,batch_size=1)\n",
    "    print(predy)\n",
    "    print(testy)\n",
    "    #realLab.append(int(testy))\n",
    "    #predLab.append(predy)\n",
    "#print(realLab,predLab)\n",
    "#print(confusion_matrix(realLab, predLab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1]\n",
      "[[10  0]\n",
      " [ 9  0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "predy = model.predict(features_x,batch_size=1).argmax(axis=-1)\n",
    "labels = labels.astype(int)\n",
    "print(labels)\n",
    "print(confusion_matrix(labels, predy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
